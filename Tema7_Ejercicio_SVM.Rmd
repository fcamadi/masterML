---
title: "Tema7_Ejercicio_SVM"
author: "Fran Camacho"
date: "2025-03-05"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Tema 7 - Ejercicio SVM 

Ejercicio equivalente al de redes de neuronas, pero usando "support vector machines".


## Paso 1 – Carga de los datos

Mismos datos que en la parte de redes de neuronas. 

```{r}
# import the CSV file
bank_raw <- read.csv(file.path("Chapter07/Bank", "bank.csv"), sep = ";", stringsAsFactors = TRUE)
```


## Paso 2 – Exploración y preparación de los datos

Importar librerías necesarias:

```{r}
#https://www.jstatsoft.org/article/view/v011i09
if (!require(kernlab)) install.packages('kernlab', dependencies = T)
library(kernlab) 

if (!require(caret)) install.packages('caret', dependencies = T)
library(caret)   

# LIBSVM   https://www.csie.ntu.edu.tw/~cjlin/libsvm/
if (!require(e1071)) install.packages('e1071', dependencies = T)
library(e1071)
```


Preparamos el dataset de igual manera que para las redes de neuronas:

```{r}
#scale numeric variables (neither day nor month)
maxs <- apply(bank_raw[c(1,6,12,13,14,15)], 2, max)
mins <- apply(bank_raw[c(1,6,12,13,14,15)], 2, min)

bank_norm <- data.frame(scale(bank_raw[c(1,6,12,13,14,15)], center = mins, scale = maxs - mins))

#hot encoding of categorical features         
dummies <- dummyVars(" ~ job + marital + education + default + housing + loan + contact + poutcome", data = bank_raw)
bank_hot_encoded_feat <-  data.frame(predict(dummies, newdata = bank_raw))

#encoding month (name to number)
month_to_number <- function(month_name) {
  month_and_number <- c("jan"=1,"feb"=2,"mar"=3,"apr"=4,"may"=5,"jun"=6,"jul"=7,"aug"=8,"sep"=9,"oct"=10,"nov"=11,"dec"=12)
  return(month_and_number[as.character(month_name)])
}
bank_raw$month_num <- sapply(bank_raw$month, month_to_number)


#put all features in the same dataframe
bank_processed <- cbind(bank_norm,as.numeric(bank_raw$day),bank_raw$month_num,bank_hot_encoded_feat,bank_raw$y)
names(bank_processed)[7:8] <- c("day","month")
names(bank_processed)[41] <- "y"
head(bank_processed,5)

```


Finalmente, creamos los conjuntos de entrenamiento y validación de igual manera que para las redes de neuronas:

```{r}
#Set seed to make the process reproducible
set.seed(9)

#partitioning data frame into training (75%) and testing (25%) sets
train_indices <- createDataPartition(bank_processed$y, times=1, p=.75, list=FALSE)

#create training set
bank_processed_train <- bank_processed[train_indices, ]

#create testing set
bank_processed_test  <- bank_processed[-train_indices, ]


#view number of rows in each set
nrow(bank_processed_train)  # 3391
nrow(bank_processed_test)   # 1130

```



## Paso 3: Entrenamiento del modelo

Vamos a comparar dos kernels de la librería kernlab.

```{r}
#train the model vanilladot
model_vanilladot <- ksvm(y ~ ., data=bank_processed_train, kernel="vanilladot")
model_vanilladot
```

```{r}
#train the model rbfdot
model_rbfdot <- ksvm(y ~ ., data=bank_processed_train, kernel="rbfdot")
model_rbfdot
```

El error es ligeramente más pequeño con el kernel rbf ("radial basis function").


[ 

EXTRA:

Probamos también con la librería LIBSVM.

```{r}
#train the model e1071
model_e1071 <- svm (y ~ ., data=bank_processed_train, scale = FALSE)  # default values:  kernel = RBF, cost = 1
model_e1071
```
]


## Paso 4 – Predicción del modelo

Con la red entrenada podemos realizar predicciones usando el dataset de validación y
obteniendo la pertinente matriz de confusión:

```{r}
#Confusion matrix vanilladot
prediction_vanilladot <- predict(model_vanilladot, bank_processed_test)
confusionMatrix(as.factor(bank_processed_test$y), as.factor(prediction_vanilladot), positive="yes", mode = "everything")
```

```{r}
#Confusion matrix rbfdot
prediction_rbfdot <- predict(model_rbfdot, bank_processed_test)
confusionMatrix(as.factor(bank_processed_test$y), as.factor(prediction_rbfdot), positive="yes", mode = "everything")
```


```{r}
#Confusion matrix e1071
prediction_e1071 <- predict(model_e1071, bank_processed_test)
confusionMatrix(as.factor(bank_processed_test$y), as.factor(prediction_e1071), positive="yes", mode = "everything")
```

Con los valores por defecto, se obtiene la misma exactitud .. pero con muchos falsos positivos 
(El modelo predice 129 positivos que en realidad son negativos. Solo predice correctamente 1 de los 130!)



## Paso 5 – Mejora del modelo

Como se explica en el libro, vamos a intentar averiguar si con algún valor del parámetro coste (parámetro C en la función ksvm), 
se puede obtener una exactitud mejor que con el valor por defecto (C=1):

```{r}
set.seed(12345)

cost_values <- c(1, seq(from = 5, to = 50, by = 5))

accuracy_values <- sapply(cost_values, function(x) {

  m <-ksvm (y ~ ., data=bank_processed_train, kernel="rbfdot", C = x)
  pred <- predict(m, bank_processed_test)

  agree <- ifelse(pred == bank_processed_test$y, 1, 0)
  accuracy <- sum(agree) / nrow(bank_processed_test)

  return (accuracy)
})

plot(cost_values, accuracy_values, type = "b")

```

El mejor resultado se obtiene para C=5.

Examinamos con más detalle los valores alrededor de 5:

```{r}
set.seed(12345)

cost_values <- c(seq(from = 2, to = 8, by = 1))

accuracy_values <- sapply(cost_values, function(x) {

  m <- ksvm(y ~ ., data=bank_processed_train, kernel="rbfdot", C = x)
  pred <- predict(m, bank_processed_test)

  agree <- ifelse(pred == bank_processed_test$y, 1, 0)
  accuracy <- sum(agree) / nrow(bank_processed_test)

  print(sprintf("C: %f - acc: %f", x, accuracy))

  return (accuracy)
})

```

La gráfica:

```{r}

plot(cost_values, accuracy_values, type = "b")

```

Aunque por muy poco, el mejor valor de la exactitud se da para C=3.
Entonces entrenamos el modelo y hacemos la predicción con ese valor.


```{r}
#train the model

model_rbfdot_C <- ksvm(y ~ ., data=bank_processed_train, kernel="rbfdot", C = 3)
model_rbfdot_C
```

C = 5

Support Vector Machine object of class "ksvm" 

SV type: C-svc  (classification) 
 parameter : cost C = 5 

Gaussian Radial Basis kernel function. 
 Hyperparameter : sigma =  0.0182756619058051 

Number of Support Vectors : 969 

Objective Function Value : -2627.335 
Training error : 0.055146 


```{r}
#Confusion matrix
prediction_rbfdot <- predict(model_rbfdot_C, bank_processed_test)
confusionMatrix(as.factor(bank_processed_test$y), as.factor(prediction_rbfdot))
```

C_= 5

Confusion Matrix and Statistics

          Reference
Prediction  no yes
       no  977  23
       yes  98  32
                                          
               Accuracy : 0.8929 


Al igual que con las redes de neuronas, parece como si hubiera un muro en el 88-89%, y no consigo pasar de esta exactitud.



[  

EXTRA:

Intentamos mejorar también el resultado del modelo **e10701** variando el parámetro coste:


```{r}
set.seed(12345)

cost_values <- c(1, seq(from = 5, to = 50, by = 5))

accuracy_values <- sapply(cost_values, function(x) {

  m <- svm(y ~ ., data=bank_processed_train, cost = x, cross = 5)
  pred <- predict(m, bank_processed_test)

  agree <- ifelse(pred == bank_processed_test$y, 1, 0)
  accuracy <- sum(agree) / nrow(bank_processed_test)

  print(sprintf("C: %f - acc: %f", x, accuracy))
    
  return (accuracy)
})

```


```{r}

plot(cost_values, accuracy_values, type = "b")

```


El mejor resultado parece ser con cost=1 ...


```{r}
set.seed(12345)

cost_values <- c(seq(from = 0.25, to = 4, by = 0.25))

accuracy_values <- sapply(cost_values, function(x) {

  m <- svm(y ~ ., data=bank_processed_train, cost = x)
  pred <- predict(m, bank_processed_test)

  agree <- ifelse(pred == bank_processed_test$y, 1, 0)
  accuracy <- sum(agree) / nrow(bank_processed_test)
  
  print(sprintf("C: %f - acc: %f", x, accuracy))

  return (accuracy)
})

```


```{r}

plot(cost_values, accuracy_values, type = "b")

```

Pues es mejor con 2.

```{r}
#train the model e1071
model_e1071_C2 <- svm(y ~ ., data=bank_processed_train, cost = 2 , scale = FALSE)
model_e1071_C2
```


```{r}
#Confusion matrix e1071
prediction_e1071 <- predict(model_e1071_C2, bank_processed_test)
confusionMatrix(as.factor(bank_processed_test$y), as.factor(prediction_e1071), positive="yes", mode = "everything")
```



]
