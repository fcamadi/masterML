---
title: "hate_speech_03_not_labelled_too_many_positives"
author: "Fran Camacho"
date: "2025-07-08"
output: word_document
---



# TFM - Clasificación de comentarios no etiquetados (1)

-- Utilizando solamente los datasets encontrados en internet --

En esta carpeta "posts" se encuentran los comentarios de lectores escritos en diversas noticias
de El Mundo del mes de junio de 2025. Se trata de noticias de politica nacional de España principalmente,
pero también hay noticias de política internacional, de deportes, o relativas a televisión.


```{r}
print(getwd())
```


### 1 - Leer los CSVs


```{r}
source('hate_speech_03_common.R')
```


```{r}
load_libraries()
```



Leemos los ficheros con los comentarios de los lectores:


```{r}
# Set the directory containing the CSV files
directory <- "./posts"

# List all CSV files in the directory
csv_files <- list.files(path = directory, pattern = "*.csv", full.names = TRUE)

# Read all CSV files into a list of data frames
#csv_data_list <- lapply(csv_files, read.csv)
csv_data_list <- lapply(csv_files, function(file) read.csv(file, sep = "|", header = FALSE))

# Combine all data frames into a single data frame
posts_elmundo_June25 <- as.data.frame(do.call(rbind, csv_data_list))

cat("Number of posts by readers: ",nrow(posts_elmundo_June25))
```

```{ r}
#Another way

total_rows <- 0

for (file in csv_files) {
  df_name <- sub("\\.[^.]*$", "", basename(file))
  print(df_name)
    
  df <- read.csv(file, sep = "|", header = FALSE)
  # Assign the data frame to a variable with the name of the file (without extension)
  assign(df_name, df, envir = .GlobalEnv)
  total_rows <- total_rows + nrow(df)
}
cat("Total rows: ",total_rows)
```


```{r}
head(posts_elmundo_June25)
```


```{ r}
write.table(posts_elmundo_June25, "posts_elmundo_June25.csv", sep = "|")
```


Renombrar nombres de variables:

```{r}
colnames(posts_elmundo_June25) <- c("author","post")
```


Eliminar columna autor (y añadir columna )


```{r}
posts_elmundo_June25$label <- NA
posts_elmundo_June25 <- posts_elmundo_June25[,-1]
```


```{r}
head(posts_elmundo_June25)
tail(posts_elmundo_June25)
```


### 2 - Carga de los modelos


Modelo 100k:

```{r}
#save model
#saveRDS(liblinear_svm_model_weights, file="svm_liblinear_all_datasets_100k_freq100_weights_1_2.rds")

# Load the model
svm_liblinear_100k <- readRDS("svm_liblinear_all_datasets_100k_freq100_weights_1_2.rds")
```


Modelo 600k:

```{r}
#save model
#saveRDS(liblinear_svm_model_weights, file="svm_liblinear_all_datasets_freq100_weights_1_3.rds")

# Load the model
svm_liblinear_600k <- readRDS("svm_liblinear_all_datasets_freq100_weights_1_3.rds")
```


Modelo "mejor" (type 2, cost 0.1, bias 10, weights 1/2):  


```{r}
#save model
#saveRDS(liblinear_svm_model_weights, file="svm_liblinear_all_datasets_100k_freq100_weights_1_2.rds")

# Load the model
svm_liblinear_100k_best <- readRDS("svm_liblinear_all_datasets_100k_type2_freq50_cost01_bias10_weights_1_2.rds")
```


### 3 - Clasificación de los comentarios nuevos


1 - Elegimos la mitad de los comentarios de manera aleatoria

Seleccionamos 2000 comentarios de los casi 3000 (con esos 2000 comentarios creamos 2 dataframes).

```{r}
set.seed(10)

# number of rows to select from the total (2960)
n <- 2000  

# Randomly select rows
posts_2000 <- posts_elmundo_June25[sample(nrow(posts_elmundo_June25),n), ]

n <- 1000
# subset1
indices <- sample(nrow(posts_2000), n)

# first subset
subset1 <- posts_2000[indices, ]

# subset2
subset2 <- posts_2000[-indices, ]

```



2 - Clasificar dichos mensajes con los dos modelos. Evaluar el resultado.


```{r}
train_vocab_100k <- colnames(svm_liblinear_100k$W)

subset1_sparse_matrix_100k <- process_unlabelled_posts(subset1, train_vocab_100k)
```

```{r}
train_vocab_600k <- colnames(svm_liblinear_600k$W)

subset1_sparse_matrix_600k <- process_unlabelled_posts(subset1, train_vocab_600k)
```


```{r}
train_vocab_100k_best <- colnames(svm_liblinear_100k_best$W)

subset1_sparse_matrix_100k_best <- process_unlabelled_posts(subset1, train_vocab_100k_best)
```


Clasificar los comentarios del subconjunto 1:

```{r}

predictions_100k <- predict(svm_liblinear_100k, subset1_sparse_matrix_100k)

#print(predictions_100k$predictions)
```

```{r}

predictions_600k <- predict(svm_liblinear_600k, subset1_sparse_matrix_600k)

#print(predictions_600k$predictions)
```

```{r}

predictions_100k_best <- predict(svm_liblinear_100k_best, subset1_sparse_matrix_100k_best)

#print(predictions_100k$predictions)
```

```{r}
subset1$label_100k <- predictions_100k$predictions
subset1$label_600k <- predictions_600k$predictions
subset1$label_100k_best <- predictions_100k_best$predictions
```


```{r}
subset1[1:100,]
```

```{r}
table(subset1$label_100k)
prop.table(table(subset1$label_100k))
```


```{r}
table(subset1$label_600k)
prop.table(table(subset1$label_600k))
```


```{r}
table(subset1$label_100k_best)
prop.table(table(subset1$label_100k_best))
```

A simple vista, considero bastante extraño (y muy mala señal) que ambos modelos hayan predicho tantos positivos/comentarios de odio.
(Y eso que conozco El Mundo -y a sus lectores- desde hace muchísimo tiempo).

He pensado examinar los comentarios, clasificarlos yo, y comparar mi valoración con los resultados de ambos modelos.




Realizamos la clasificación de los comentarios (columna "label"), y volvemos a leer el csv.
Comparamos mi evaluación con las predicciones de los 3 modelos:


```{r}
result1_1000 <- read.csv2("subset1_classification_SVM_100k_and_600k.csv", sep = "|", header = TRUE)
```


```{r}
result1_1000$label_100k_best <- subset1$label_100k_best

write.table(result1_1000, "subset1_classification_SVM_100k_600k_and_100kbest.csv", sep = "|")
```


### Evaluación de los modelos

Matriz de confusión modelo 100k:

```{r}
#Confusion matrix
confusionMatrix(reference = as.factor(result1_1000$label), 
                data = as.factor(result1_1000$label_100k), 
                positive="1", 
                mode = "everything")
```


Matriz de confusión modelo 600k:

```{r}
#Confusion matrix
confusionMatrix(reference = as.factor(result1_1000$label), 
                data = as.factor(result1_1000$label_600k), 
                positive="1", 
                mode = "everything")
```

Matriz de confusión modelo 100k "mejor":

```{r}
#Confusion matrix
confusionMatrix(reference = as.factor(result1_1000$label), 
                data = as.factor(result1_1000$label_100k_best), 
                positive="1", 
                mode = "everything")
```



Los resultados son muy muy malos. Curiosamente, el modelo que parecía mejor entrenado (el de 100k observaciones),
obtiene unos resultados terribles, todavía peores que el modelo que fue entrenado con el dataset completo (que obtiene al menos un kappa de 0.2). 


Posibles motivos:

- Los comentarios de los lectores de estas noticias, son demasiado diferentes a los contenidos en los datasets encontrados (Hatemedia, HuggingFace y Kaggle)

- La manera de entrenar los modelos ha hecho que se haya producido un sobreajuste


Posibles soluciones:

- clasificar más comentarios de lectores de El Mundo y añadirlos a los datasets usados (datasets totales de 100k y 600k observaciones).

- entrenar un SVM solo con estos comentarios



Finalmente comparar los resultados de ambas opciones.





