---
title: "Model_Evaluation"
author: "Fran Camacho"
date: "2025-03-30"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Evaluación de modelos


[Tomado del libro "Machine Learning with R", de Brett Lantz].


```{r}

if (!require(mltools)) install.packages('mltools', dependencies = T)
library(mltools)

if (!require(caret)) install.packages('caret', dependencies = T)
library(caret) 

```


### Matriz de confusión


```{r}
#Taken from Chapter 3
#CrossTable/ConfusionMatrix
#CrossTable(x = pima_norm_test_labels, y = pima_pred, prop.chisq = FALSE)
```

   Cell Contents
|-------------------------|
|                       N |
|           N / Row Total |
|           N / Col Total |
|         N / Table Total |
|-------------------------|

 
Total Observations in Table:  159 

 
                      | pima_pred 
pima_norm_test_labels |  Negative |  Positive | Row Total | 
----------------------|-----------|-----------|-----------|
             Negative |        94 |        12 |       106 | 
                      |     0.887 |     0.113 |  **0.667**| 
                      |     0.810 |     0.279 |           | 
                      |  **0.591**|     0.075 |           | 
----------------------|-----------|-----------|-----------|
             Positive |        22 |        31 |        53 | 
                      |     0.415 |     0.585 |  **0.333**| 
                      |     0.190 |     0.721 |           | 
                      |     0.138 |  **0.195**|           | 
----------------------|-----------|-----------|-----------|
         Column Total |       116 |        43 |       159 | 
                      |  **0.730**|  **0.270**|           | 
----------------------|-----------|-----------|-----------|

```{r}
TP <- 31
TN <- 94
FP <- 12
FN <- 22
```



### Exactitud


#accuracy = TP + TN / TP + FP + TN + FN =  TP + TN / total = 31 + 94 / 31 + 12 + 94 + 22 = 31 + 94 / 159 = 0.7862 
accuracy = TP + TN / TP + FP + TN + FN =  TP + TN / total = 31 + 94 / 31 + 12 + 94 + 22 = 31 + 94 / 159 = 0.7862 

```{r}
accuracy <- (TP + TN) / (TP + FP + TN + FN)
accuracy
```


### Coeficiente **kappa** 

https://es.wikipedia.org/wiki/Coeficiente_kappa_de_Cohen

El Coeficiente kappa de Cohen es una medida estadística que ajusta el efecto del azar en la proporción de la concordancia observada.
En general se cree que es una medida más robusta que el simple cálculo del porcentaje de concordancia, ya que κ tiene en cuenta el acuerdo que ocurre por azar.

La Kappa de Cohen es una medida de la fiabilidad con la que dos 'evaluadores' miden lo mismo. 

k = (Pr(a) - Pr(e)) / (1 - Pr(e))

Donde

**Pr(a)** ess el acuerdo observado relativo entre los observadores
**Pr(e)** es la probabilidad hipotética de acuerdo por azar, utilizando los datos observados para calcular las probabilidades
de que cada observador clasifique aleatoriamente cada categoría.

Si los evaluadores están completamente de acuerdo, entonces κ = 1.
Si no hay acuerdo entre los calificadores distinto al que cabría esperar por azar (según lo definido por Pr(e)), κ = 0. 


```{r}

pr_a <- 0.591 + 0.195
pr_a
pr_e = 0.667*0.730 + 0.333*0.270  # probability of both positive + probability of both negative
pr_e

```


```{r}

k = (pr_a - pr_e) / (1 - pr_e)
k

```


## Coeficiente de correlación de Matthews


MCC = [TP × TN − FP × FN]  /  [√(TP + FP)(TP + FN)(TN + FP)(TN + FN)]

```{r}

TPmultTNminusFPmultFN <- (TP * TN) - (FP * FN)
denom <- sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))
MCC <- TPmultTNminusFPmultFN / denom
MCC

```

"The mltools package by Ben Gorman provides an mcc() function which can perform the MCC
calculation using vectors of predicted and actual values."

```{r}
#Chapter 3:
#CrossTable(x = pima_norm_test_labels, y = pima_pred, prop.chisq = FALSE) ->
mcc(pima_norm_test_labels, pima_pred)
```

Mismo resultado claro :)

"Alternatively, for a binary classifier where the positive class is coded as 1 and the negative class
is coded as 0, the MCC is identical to the Pearson correlation between the predicted and actual
values:"

```{r}
#Chapter 3:
#CrossTable(x = pima_norm_test_labels, y = pima_pred, prop.chisq = FALSE) ->
cor( ifelse(pima_norm_test_labels == 'Positive',1,0), 
     ifelse(pima_pred == 'Positive',1,0) )
```

" [...] There may be no single metric that better captures the overall performance of
a binary classification model. However, as you will soon see, a more in-depth understanding of
model performance can be obtained using combinations of metrics."



## Sensitivity and specificity


"Finding a useful classifier often involves a balance between predictions that are overly conserva-
tive and overly aggressive. For example, an email filter could guarantee to eliminate every spam
message by aggressively filtering nearly every ham message. On the other hand, to guarantee
that no ham messages will be inadvertently filtered might require us to allow an unacceptable
amount of spam to pass through the filter. A pair of performance measures captures this tradeoff:
sensitivity and specificity."

```{r}
sensitivity <- TP / (TP+FN)
sensitivity
```

```{r}
specifity <- TN / (TN+FP)
specifity
```



## Precision and recall

"Closely related to sensitivity and specificity are two other performance measures related to com-
promises made in classification: precision and recall. 

Used primarily in the context of information retrieval, these statistics are intended to indicate 
how interesting and relevant a model’s results are, or whether the predictions are diluted by meaningless noise."

```{r}
precision <- TP / (TP+FP)
precision
```

```{r}
recall <- TP / (TP+FN)
recall
```



See:

https://en.wikipedia.org/wiki/Sensitivity_and_specificity



