---
title: "hate_speech_02_hatemedia"
author: "Fran Camacho"
date: "2025-06-04"
output: word_document
---


# TFM - Procesar dataset HuggingFace - SVM


```{r}
source('hate_speech_common.R')
```


Carga de paquetes que son necesarios para diversas funciones.

```{r}
load_libraries()    
```



# Paso 1: obtención del dataset y procesado preliminar


```{r}
# import the CSV file
odio_huggingface_raw <- read.csv(file.path("dataset_02_huggingface.csv"), sep=";")  
odio_huggingface_raw <- odio_huggingface_raw[-1]     # -> ~29000 obs. of 2 variables
```


Estructura del dataset:

```{r}
str(odio_huggingface_raw)
```

La columna "label" es de tipo int Ya que se trata en realidad de una variable categórica 0/1, es conveniente transformarla en un factor:

```{r}
#Convert class into a factor
odio_huggingface_raw$label <- factor(odio_huggingface_raw$label)
```


Examinamos el resultado

```{r}
table(odio_huggingface_raw$label)
prop.table(table(odio_huggingface_raw$label))
```


```{r}
head(odio_huggingface_raw)
```

```{r}
tail(odio_huggingface_raw)
```

# Paso 2: procesado del dataset


Como primer paso, se van a eliminar que referencien a otros usuarios en los comentarios. 
Estos son referenciados mediante la "@".

También, con ayuda de la librería cleantext examinamos los comentarios y haremos un primer procesado:


```{r}
odio_huggingface_raw[2,1]
```

```{r}
#check_text(odio_kaggle_raw$post)
system.time({
  odio_huggingface <- preprocess_posts(odio_huggingface, odio_huggingface_raw)
})
```

Comprobamos el resultado:

```{r}
odio_huggingface[2,1]
```


### Corpus

Creación del objeto corpus con todas los mensajes:

```{r}
#create corpus
posts_corpus <- VCorpus(VectorSource(odio_huggingface$post))

print(posts_corpus)
```

Limpieza de los textos del corpus:

```{r}
system.time({

  posts_corpus_clean <- clean_corpus(posts_corpus)

})
```

Se examina el corpus y no se aprecia nada raro.

```{r}
#View(posts_corpus_clean)
```


Finalmente, se procede a la **"tokenización"** de los comentarios:

```{r}
system.time({
  
  posts_dtm <- DocumentTermMatrix(posts_corpus_clean)
  
  posts_dtm
})
```


Ahora hay que crear los conjuntos de entrenamiento y de test.


```{r}
#Set seed to make the process reproducible
set.seed(123)

result <- train_test_split(odio_huggingface, posts_dtm, 0.75)

#create training set
posts_dtm_train <-  result$dtm_train 

#create testing set
posts_dtm_test  <- result$dtm_test

#create labels sets
posts_train_labels <- result$train_labels
posts_test_labels <- result$test_labels

```


Vamos a comprobar si se mantiene la (des)proporción mensajes no de odio/mensajes de odio:

```{r}
prop.table(table(posts_train_labels))
```

```{r}
prop.table(table(posts_test_labels))
```


Se necesita ahora obtener un listado con las palabras más utilizadas:


```{r}
# Data preparation – creating indicator features for frequent words
# the function findFreqTerms() in the tm package takes a DTM and returns a character vector containing words that appear at least a  minimum number of times
posts_freq_words_train <- findFreqTerms(posts_dtm_train, 20)  #   10 -> ~3700 terms
                                                               #  29 -> ~2000
                                                               #  100 ->  ~450
print("tonto" %in% posts_freq_words_train)    
print("imbecil" %in% posts_freq_words_train)  # <- FALSE with freq 1000
print("cabron" %in% posts_freq_words_train)   # <- FALSE with freq  150
```


Y ahora utilizamos ese listado para limitar el número de columnas/features tanto del conjuntos de entrenamiento como del de test:

```{r}
dim(posts_dtm_train)
posts_dtm_freq_train <- posts_dtm_train[ , posts_freq_words_train]
dim(posts_dtm_freq_train)

dim(posts_dtm_test)
posts_dtm_freq_test <- posts_dtm_test[ , posts_freq_words_train]     
dim(posts_dtm_freq_test)
```




## Paso 3: Entrenamiento del modelo


Tanto caret como LIBSVM (e1071) han demostrado ser muy poco eficientes a la hora de tratar dataset de estos tamaños.

Utilizamos por tanto LiblineaR también con este dataset.



### LiblineaR


```{r}
# dtm -> matrix

posts_freq_train_mat <- as.matrix(posts_dtm_freq_train)
#posts_freq_train_df <- as.data.frame(posts_freq_train_mat)

posts_freq_test_mat <- as.matrix(posts_dtm_freq_test)
#posts_freq_test_df <- as.data.frame(posts_freq_test_mat)
```





```{r}
system.time({

  liblinear_svm_model <- LiblineaR(data = posts_freq_train_mat, target = posts_train_labels, type = 3)  # C = 1, not the found with heuristicC

})
```


"We recommend users

1. Try the default dual-based solver first.
2. If it is slow, check primal-based solvers.

To choose between using L1 and L2 regularization, we recommend trying L2 first unless
users need a sparse model."


```{r}
# prediction
system.time({
  
  prediction_liblinear <- predict(liblinear_svm_model, posts_freq_test_mat)

})
```


```{r}
#Confusion matrix
confusionMatrix(reference = as.factor(posts_test_labels), data = as.factor(prediction_liblinear$predictions), positive="1", mode = "everything")
```

Resultados:

**freq = 100**

Confusion Matrix and Statistics

          Reference
Prediction    0    1
         0 5385 1249
         1  262  567
                                          
               Accuracy : 0.7975          
                 95% CI : (0.7882, 0.8066)
    No Information Rate : 0.7567          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.3259          
                                          
 Mcnemar's Test P-Value : < 2.2e-16       
                                          
            Sensitivity : 0.31222         
            Specificity : 0.95360         
         Pos Pred Value : 0.68396         
         Neg Pred Value : 0.81173         
              Precision : 0.68396         
                 Recall : 0.31222         
                     F1 : 0.42873         
             Prevalence : 0.24333         
         Detection Rate : 0.07597         
   Detection Prevalence : 0.11108         
      Balanced Accuracy : 0.63291         
                                          
       'Positive' Class : 1 
       

Este resultado es algo peor que el obtenido con el dataset de hatemedia.

**freq = 50**

Confusion Matrix and Statistics

          Reference
Prediction    0    1
         0 5346  999
         1  301  817
                                         
               Accuracy : 0.8258         
                 95% CI : (0.817, 0.8344)
    No Information Rate : 0.7567         
    P-Value [Acc > NIR] : < 2.2e-16      
                                         
                  Kappa : 0.456          
                                         
 Mcnemar's Test P-Value : < 2.2e-16      
                                         
            Sensitivity : 0.4499         
            Specificity : 0.9467         
         Pos Pred Value : 0.7308         
         Neg Pred Value : 0.8426         
              Precision : 0.7308         
                 Recall : 0.4499         
                     F1 : 0.5569         
             Prevalence : 0.2433         
         Detection Rate : 0.1095         
   Detection Prevalence : 0.1498         
      Balanced Accuracy : 0.6983         
                                         
       'Positive' Class : 1    

Aumentando el número de términos, se mejora algo el resultado  ...
(Bajando a 10, no. Empeora).


**freq = 20**

Confusion Matrix and Statistics

          Reference
Prediction    0    1
         0 5216  845
         1  431  971
                                          
               Accuracy : 0.829           
                 95% CI : (0.8203, 0.8375)
    No Information Rate : 0.7567          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.4968          
                                          
 Mcnemar's Test P-Value : < 2.2e-16       
                                          
            Sensitivity : 0.5347          
            Specificity : 0.9237          
         Pos Pred Value : 0.6926          
         Neg Pred Value : 0.8606          
              Precision : 0.6926          
                 Recall : 0.5347          
                     F1 : 0.6035          
             Prevalence : 0.2433          
         Detection Rate : 0.1301          
   Detection Prevalence : 0.1879          
      Balanced Accuracy : 0.7292          
                                          
       'Positive' Class : 1 



   
- Intentamos mejorar el modelo usando el coste calculado por  heuristicC:   
      
```{r}
c <- heuristicC(posts_freq_train_mat)

c
```

c <- 0.289722      
      
      
No se mejora el resultado obtenido con coste 1:
(Con coste 1.5 sale muy parecido ...?)


- Intentamos mejorar el modelo usando pesos, para favorecer la clase minoritaria:


```{r}
# Define class weights
class_weights <- c("0" = 1, "1" = 3)  # Assign higher weight to the minority class
```



```{r}
system.time({

  liblinear_svm_model_weights <- LiblineaR(data = posts_freq_train_mat, target = posts_train_labels, type = 3, cost = 1,
                                   wi = class_weights)

  prediction_liblinear_weights <- predict(liblinear_svm_model_weights, posts_freq_test_mat)
})
```


```{r}
#Confusion matrix
confusionMatrix(reference = as.factor(posts_test_labels), data = as.factor(prediction_liblinear_weights$predictions), positive="1", mode = "everything")
```

- freq 20, coste =  1, pesos 1/3

Confusion Matrix and Statistics

          Reference
Prediction    0    1
         0 4474  487
         1 1173 1329
                                        
               Accuracy : 0.7776        
                 95% CI : (0.768, 0.787)
    No Information Rate : 0.7567        
    P-Value [Acc > NIR] : 1.143e-05     
                                        
                  Kappa : 0.4646        
                                        
 Mcnemar's Test P-Value : < 2.2e-16     
                                        
            Sensitivity : 0.7318        
            Specificity : 0.7923        
         Pos Pred Value : 0.5312        
         Neg Pred Value : 0.9018        
              Precision : 0.5312        
                 Recall : 0.7318        
                     F1 : 0.6156        
             Prevalence : 0.2433        
         Detection Rate : 0.1781        
   Detection Prevalence : 0.3353        
      Balanced Accuracy : 0.7621        
                                        
       'Positive' Class : 1   
       

- freq 20, coste =  0.289722, pesos 2/3

Confusion Matrix and Statistics

          Reference
Prediction    0    1
         0 5002  694
         1  645 1122
                                          
               Accuracy : 0.8206          
                 95% CI : (0.8117, 0.8292)
    No Information Rate : 0.7567          
    P-Value [Acc > NIR] : <2e-16          
                                          
                  Kappa : 0.5083          
                                          
 Mcnemar's Test P-Value : 0.1896          
                                          
            Sensitivity : 0.6178          
            Specificity : 0.8858          
         Pos Pred Value : 0.6350          
         Neg Pred Value : 0.8782          
              Precision : 0.6350          
                 Recall : 0.6178          
                     F1 : 0.6263          
             Prevalence : 0.2433          
         Detection Rate : 0.1503          
   Detection Prevalence : 0.2368          
      Balanced Accuracy : 0.7518          
                                          
       'Positive' Class : 1   


Con freq=20, pesos 2/3, coste el de la heurística, la exactitud es casi igual, y se consigue mejorar kappa ligeramente.

La exactitud roza el 82.5%, kappa el 0.51
       
       
       
       






