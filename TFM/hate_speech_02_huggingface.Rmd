---
title: "hate_speech_02_hatemedia"
author: "Fran Camacho"
date: "2025-06-04"
output: word_document
---


# TFM - Procesar dataset HuggingFace


Carga de paquetes que son necesarios para diversas funciones.

```{r}
if (!require(tm)) install.packages('tm', dependencies = T)   # text mining
library(tm)

if (!require(SnowballC)) install.packages('SnowballC', dependencies = T)   # stemming
library(SnowballC)

if (!require(caret)) install.packages('caret', dependencies = T)   # data partitioning, confusion matrix
library(caret)         

if (!require(e1071)) install.packages('e1071', dependencies = T)  #LibSVM
library(e1071)  

# Liblinear instead of LibSVM
#
# https://www.csie.ntu.edu.tw/~cjlin/liblinear/
#
# https://cran.r-project.org/web/packages/LiblineaR/
if (!require(LiblineaR)) install.packages('LiblineaR', dependencies = T)
library(LiblineaR)
```



# Paso 1: obtención del dataset y procesado


```{r}
# import the CSV file
odio_huggingface <- read.csv(file.path("dataset_02_huggingface.csv"), sep=";")  
odio_huggingface <- odio_huggingface[-1]     # -> 574272 obs. of 2 variables
```


Estructura del dataset:

```{r}
str(odio_huggingface)
```

La columna "label" es de tipo int Ya que se trata en realidad de una variable categórica 0/1, es conveniente transformarla en un factor:

```{r}
#Convert class into a factor
odio_huggingface$label <- factor(odio_huggingface$label)
```


Ya sabíamos que se trata de un dataset muy, muy desbalanceado:

```{r}
table(odio_huggingface$label)
```

```{r}
prop.table(table(odio_huggingface$label))
```



# Paso 2: procesado del dataset

Como primer paso, se van a eliminar que referencien a otros usuarios en los comentarios. Estos son referenciados mediante la "@".
Por ejemplo "@USER".


```{r}
odio_huggingface[2,1]
```

Eliminar referencias:

```{r}
#remove references to other posters (marked with @)
odio_huggingface$post <- gsub("@\\w+", "", odio_huggingface$post) 
```

Comprobación:

```{r}
odio_huggingface[2,1]
```



Hay que borrar también los emoticonos, y los teléfonos



```{r}
tail(odio_huggingface,100)[1]
```


Creación del objeto corpus con todas los mensajes:

```{r}
#create corpus
posts_corpus <- VCorpus(VectorSource(odio_huggingface$post))
print(posts_corpus)
```


Limpieza de los textos:

Procesado habitual: eliminar mayúsculas, números ...

```{r}
system.time({

#To lowercase 
posts_corpus_clean <- tm_map(posts_corpus, content_transformer(tolower))

#Remove numbers
posts_corpus_clean <- tm_map(posts_corpus_clean, removeNumbers)

#Remove stopwords
# check words and languages with ?stopwords
posts_corpus_clean <- tm_map(posts_corpus_clean, removeWords, stopwords()) 

#Remove punctuation signs
posts_corpus_clean <- tm_map(posts_corpus_clean, removePunctuation) 

})
```


Stemming dataset completo:

```{r}
system.time({

#Carry out the stemming:
# To apply the wordStem() function to an entire corpus of text documents, the tm package includes
# the stemDocument() transformation.
posts_corpus_clean <- tm_map(posts_corpus_clean, stemDocument)

#Finally eliminate unneeded whitespace produced by previous steps
posts_corpus_clean <- tm_map(posts_corpus_clean, stripWhitespace) 

})
```

Unos 2


```{r}
#Check the final result (output omitted for brevity)
#before cleaning
i <- 29760
print(as.character(posts_corpus[[i]]))
#after
print(as.character(posts_corpus_clean[[i]]))
```




Finalmente, se procede a la **"tokenización"** de los comentarios:

```{r}
system.time({
  
  posts_dtm <- DocumentTermMatrix(posts_corpus_clean)
  posts_dtm
  
})
```


Ahora hay que crear los conjuntos de entrenamiento y de test.


```{r}
#Set seed to make the process reproducible
set.seed(123)

#partitioning data frame into training (75%) and testing (25%) sets
train_indices <- createDataPartition(odio_huggingface$label, times=1, p=.75, list=FALSE)

#create training set
posts_dtm_train <- posts_dtm[train_indices, ]

#create testing set
posts_dtm_test  <- posts_dtm[-train_indices, ]

#create labels sets
posts_train_labels <- odio_huggingface[train_indices, ]$label
posts_test_labels <- odio_huggingface[-train_indices, ]$label

#view number of rows in each set
nrow(posts_dtm_train)  # 
nrow(posts_dtm_test)   # 
length(posts_train_labels)  # 
length(posts_test_labels)   # 
```

Vamos a comprobar si se mantiene la (des)proporción mensajes no de odio/mensajes de odio:

```{r}
prop.table(table(posts_train_labels))
```

```{r}
prop.table(table(posts_test_labels))
```


Se necesita ahora obtener un listado con las palabras más utilizadas:


```{r}
# Data preparation – creating indicator features for frequent words
# the function findFreqTerms() in the tm package takes a DTM and returns a character vector containing words that appear at least a  minimum number of times
posts_freq_words <- findFreqTerms(posts_dtm_train, 20)  #   100 -> ~17000 terms
                                                          #   500 ->  ~6000
                                                          #  1000 ->  ~3600 
print("tonto" %in% posts_freq_words)    
print("imbecil" %in% posts_freq_words)  # <- FALSE with freq 1000
print("cabron" %in% posts_freq_words)   # <- FALSE with freq  150
```

Vamos a probar primero usando los términos que aparecen al menos 100 veces.


Y ahora utilizamos ese listado para limitar el número de columnas/features de los conjuntos de entrenamiento y de test:

```{r}
ncol(posts_dtm_train)
#[1] 24951
posts_dtm_freq_train <- posts_dtm_train[ , posts_freq_words]
posts_dtm_freq_test <- posts_dtm_test[ , posts_freq_words]
ncol(posts_dtm_freq_train)
#[1] 1027
```




## Paso 3: Entrenamiento del modelo


Se descarta intentar mejorar el modelo LIBSVM con tune. No tiene sentido. 
Se ha visto que para este tipo de problema, LIBSVM no es muy bueno.

LiblineaR es mucho más eficiente.



### LiblineaR


```{r}
# dtm to df

posts_freq_train_mat <- as.matrix(posts_dtm_freq_train)
posts_freq_train_df <- as.data.frame(posts_freq_train_mat)

posts_freq_test_mat <- as.matrix(posts_dtm_freq_test)
posts_freq_test_df <- as.data.frame(posts_freq_test_mat)
```



```{r}
c <- heuristicC(posts_freq_train_df)

c
```

c <-  0.3438449




```{r}
system.time({

  liblinear_svm_model <- LiblineaR(data = posts_freq_train_df, target = posts_train_labels, type = 3)  # C = 1, not the found with heuristicC
  #liblinear_svm_model <- LiblineaR(data = posts_freq_train_df, target = posts_train_labels, type = 3,  cost = 1.5,) # not better

})
```

Este modelo tarda 14-15 s en entrenar con el conjunto 100k !!!  (type = 1)(dual)  
Este modelo tarda 4 s en entrenar con el conjunto 100k !!!  (type = 2)  (primal) 
Este modelo tarda 2 s en entrenar con el conjunto 100k !!!  (type = 3) (dual)
Este modelo tarda 4 s en entrenar con el conjunto 100k !!! (type = 5) (primal?)


"We recommend users

1. Try the default dual-based solver first.
2. If it is slow, check primal-based solvers.

To choose between using L1 and L2 regularization, we recommend trying L2 first unless
users need a sparse model."


```{r}
# prediction
system.time({
  
  prediction_liblinear <- predict(liblinear_svm_model, posts_freq_test_df)

})
```

```{r}
table(as.factor(prediction_liblinear$predictions))
```



```{r}
#Confusion matrix
confusionMatrix(reference = as.factor(posts_test_labels), data = as.factor(prediction_liblinear$predictions), positive="1", mode = "everything")
```

Resultados:

**freq = 100**

Confusion Matrix and Statistics

          Reference
Prediction    0    1
         0 5385 1249
         1  262  567
                                          
               Accuracy : 0.7975          
                 95% CI : (0.7882, 0.8066)
    No Information Rate : 0.7567          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.3259          
                                          
 Mcnemar's Test P-Value : < 2.2e-16       
                                          
            Sensitivity : 0.31222         
            Specificity : 0.95360         
         Pos Pred Value : 0.68396         
         Neg Pred Value : 0.81173         
              Precision : 0.68396         
                 Recall : 0.31222         
                     F1 : 0.42873         
             Prevalence : 0.24333         
         Detection Rate : 0.07597         
   Detection Prevalence : 0.11108         
      Balanced Accuracy : 0.63291         
                                          
       'Positive' Class : 1 
       

Este resultado es algo peor que el obtenido con el dataset de hatemedia.

**freq = 50**

Confusion Matrix and Statistics

          Reference
Prediction    0    1
         0 5346  999
         1  301  817
                                         
               Accuracy : 0.8258         
                 95% CI : (0.817, 0.8344)
    No Information Rate : 0.7567         
    P-Value [Acc > NIR] : < 2.2e-16      
                                         
                  Kappa : 0.456          
                                         
 Mcnemar's Test P-Value : < 2.2e-16      
                                         
            Sensitivity : 0.4499         
            Specificity : 0.9467         
         Pos Pred Value : 0.7308         
         Neg Pred Value : 0.8426         
              Precision : 0.7308         
                 Recall : 0.4499         
                     F1 : 0.5569         
             Prevalence : 0.2433         
         Detection Rate : 0.1095         
   Detection Prevalence : 0.1498         
      Balanced Accuracy : 0.6983         
                                         
       'Positive' Class : 1    

Aumentando el número de términos, se mejora algo el resultado  ...
(Bajando a 10, no. Empeora).

**freq = 20**

Confusion Matrix and Statistics

          Reference
Prediction    0    1
         0 5255  860
         1  392  956
                                          
               Accuracy : 0.8322          
                 95% CI : (0.8236, 0.8407)
    No Information Rate : 0.7567          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.5008          
                                          
 Mcnemar's Test P-Value : < 2.2e-16       
                                          
            Sensitivity : 0.5264          
            Specificity : 0.9306          
         Pos Pred Value : 0.7092          
         Neg Pred Value : 0.8594          
              Precision : 0.7092          
                 Recall : 0.5264          
                     F1 : 0.6043          
             Prevalence : 0.2433          
         Detection Rate : 0.1281          
   Detection Prevalence : 0.1806          
      Balanced Accuracy : 0.7285          
                                          
       'Positive' Class : 1  



   
- Intentamos mejorar el modelo usando el coste calculado por  heuristicC:   
      
No se mejora el resultado obtenido con coste 1:
(Con coste 1.5 sale muy parecido ...?)


- Intentamos mejorar el modelo usando pesos, para favorecer la clase minoritaria:


```{r}
# Define class weights
class_weights <- c("0" = 2, "1" = 3)  # Assign higher weight to the minority class
```



```{r}
system.time({

  liblinear_svm_model <- LiblineaR(data = posts_freq_train_df, target = posts_train_labels, type = 3,
                                   wi = class_weights)

})
```


```{r}
# prediction
system.time({
  
  prediction_liblinear <- predict(liblinear_svm_model, posts_freq_test_df)

})
```

```{r}
#Confusion matrix
confusionMatrix(reference = as.factor(posts_test_labels), data = as.factor(prediction_liblinear$predictions), positive="1", mode = "everything")
```

Con freq=100, pesos 1/2, coste 1.5, la exactitud es la misma, y se consigue mejorar kappa ligeramente.

Con freq=20, no se consigue mejorar el resultado usando pesos.


De todas maneras los valores obtenidos no son increíblemente buenos con este dataset

exactitud rozando el 83%, kappa el 0.5
       





