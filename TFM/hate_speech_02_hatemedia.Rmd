---
title: "hate_speech_02_hatemedia"
author: "Fran Camacho"
date: "2025-06-04"
output: word_document
params:
  complete: FALSE 
---


# TFM - Procesar dataset hatemedia - SVM


```{r}
complete_dataset <- params$complete 
```


```{r}
source('hate_speech_common.R')
```


Carga de paquetes que son necesarios para diversas funciones.

```{r}
load_libraries()    
```


# Paso 1: obtención del dataset y procesado


```{r}
# import the CSV file
odio_hatemedia_raw <- read.csv(file.path("dataset_01_hatemedia.csv"), sep=";")  
odio_hatemedia_raw <- odio_hatemedia_raw[-1]     # -> 574272 obs. of 2 variables
```


Se ha comprobado que este dataset es demasiado grande para el equipo en el que se ejecuta el programa.
Se ha considerado también aplicar "downsampling". El resultado era un dataset de 22128 observaciones (completamente equilibrado).
Se ha optado por una solución intermedia: elegir de manera aleatoria un número tal de mensajes "no odio" que sumados 
a los de odio den un total de 100.000 observaciones.


```{r}
# Prepare a bit smaller dataset:  200k ?
if (!complete_dataset) {
  print("Preparing a smaller dataset")

  df_hate <- odio_hatemedia_raw[odio_hatemedia_raw$label == 1, ]
  df_no_hate <- odio_hatemedia_raw[odio_hatemedia_raw$label == 0, ]

  n  <- nrow(df_no_hate)
  k  <- 88936  # number of random rows: 18936 (quite balanced) 88936 (total 100k, best outcome?)  288936 (total 300k) 

  ids <- sample(n,size = k, replace = FALSE)
  df_no_hate_sample <- df_no_hate[ids, ,drop = FALSE]

  # join sample dataset with no hate obs. with hate obs.
  odio_hatemedia_sample_k <- rbind(df_no_hate_sample, df_hate)

  #so we don't have to change variable names
  odio_hatemedia_raw <- odio_hatemedia_sample_k
  
} else {
  print("Working with the whooole dataset. Be patient!!!")
}

```



Estructura del dataset:

```{r}
str(odio_hatemedia_raw)
```

La columna "label" es de tipo int. Ya que se trata en realidad de una variable categórica 0/1, es conveniente transformarla en un factor:

```{r}
#Convert class into a factor
odio_hatemedia_raw$label <- factor(odio_hatemedia_raw$label)
```


En la variable "post", reemplazamos las "," por espacios. Si no hacemos esto con este primer dataset, lo que se obtiene más tarde es una única cadena enorme que contiene todas las palabras del comentario. (Esto es debido a que este dataset de hatemedia ya estaba parcialmente procesado).

```{r}
# Replace all "," in this dataset. If not, after processing it we get lines of only one "huge word"
odio_hatemedia_raw$post <- gsub(',',' ',odio_hatemedia_raw$post)
```

Comprobamos:

```{r}
head(odio_hatemedia_raw,)
```

Ya sabíamos que se trata de un dataset muy, muy desbalanceado:

```{r}
table(odio_hatemedia_raw$label)
```

```{r}
prop.table(table(odio_hatemedia_raw$label))
```



# Paso 2: procesado del dataset

[

NOTA: se han detectado mensajes en catalán en este dataset:

```{r}
#odio_hatemedia_raw[17:37,1]
```

[1] "dimecr coolhunter preguntavar responsabilitat tenir trio mixt presentadors campanadser tot venir despr toni cruany ..."
[2] " avui podem jugar mama tots castellans  aixi  lamentar mes vegada  fills jordi pujol marta ferrusola segons explicar ..."
...

El objetivo de este TFM es detectar mensajes de odio en un solo idioma, no en varios, aunque sean tan parecidos como son el Castellano y el Catalán.

Dada la dificultad de detectar todos los textos en Catalán en un dataset tan grande como este, y dado el hecho de que en los medios online españoles
muchos ciudadanos catalanes escriben sus comentarios en su idioma materno, por ahora decido no eliminar estos mensajes.

]


Como primer paso, se van a eliminar que referencien a otros usuarios en los comentarios. 
Estos son referenciados mediante la "@".

También, con ayuda de la librería cleantext examinamos los comentarios y haremos un primer procesado:

Estado inicial:

```{r}
odio_hatemedia_raw[2012:2020,1]
```

```{r}
#check_text(odio_kaggle_raw$post)
system.time({
  odio_hatemedia <- preprocess_posts(odio_hatemedia, odio_hatemedia_raw)
})
```

426.444   0.586 426.994
7 minutos ...
 

Comprobamos el resultado:

```{r}
odio_hatemedia[which(odio_hatemedia$post == "pagar euro leer papanata" ), ]
#odio_hatemedia[match("pagar euro leer papanata",odio_hatemedia$post),]
```

```{r}
odio_hatemedia[1996:2004,1]
```

```{r}
# Number of rows deleted:
nrow(odio_hatemedia_raw)-nrow(odio_hatemedia)
```

Se han eliminado 3404 líneas al procesar el dataset.


**Corpus de los textos**

Ya se puede proceder a la creación del objeto corpus con todas los mensajes:

```{r}
#create corpus
system.time({
  posts_corpus <- VCorpus(VectorSource(odio_hatemedia$post))
})

print(posts_corpus)
```


Limpieza de los textos del corpus:

Procesado habitual: eliminar mayúsculas, números ...

```{r}
system.time({

  posts_corpus_clean <- clean_corpus(posts_corpus)

})
```

298.890   3.747 303.212 

5 min. aprox.


```{r}
#Check the final result (output omitted for brevity)
#before cleaning
i <- 2000
print(as.character(posts_corpus[[i]]))
#after
print(as.character(posts_corpus_clean[[i]]))
```

Como este dataset ya estaba parcialmente procesado, el resultado final es bastante parecido al estado inicial.


Finalmente, se procede a la **"tokenización"** de los comentarios:

```{r}
system.time({
  
  posts_dtm <- DocumentTermMatrix(posts_corpus_clean)
  posts_dtm
  
})
```

116.379   0.882 117.310 
Casi 2 minutos.

[

En este punto eliminar los objetos "corpus". Son casi 3 Gb cada uno y ya no son necesarios. <- con el dataset completo

```{r}
rm(posts_corpus)
rm(posts_corpus_clean)
```
  
]


Ahora hay que crear los conjuntos de entrenamiento y de test.


```{r}
#Set seed to make the process reproducible
set.seed(123)

result <- train_test_split(odio_hatemedia, posts_dtm, 0.75)

#create training set
posts_dtm_train <-  result$dtm_train 

#create testing set
posts_dtm_test  <- result$dtm_test

#create labels sets
posts_train_labels <- result$train_labels
posts_test_labels <- result$test_labels

```


Vamos a comprobar si se mantiene la (des)proporción mensajes no de odio/mensajes de odio:

```{r}
prop.table(table(posts_train_labels))
```

```{r}
prop.table(table(posts_test_labels))
```

Se necesita ahora obtener un listado con las palabras más utilizadas:


```{r}
# Data preparation – creating indicator features for frequent words
# the function findFreqTerms() in the tm package takes a DTM and returns a character vector containing words that appear at least a  minimum number of times
posts_freq_words_train <- findFreqTerms(posts_dtm_train, 100)  #   100 -> ~17000 terms
                                                          #   500 ->  ~3700
                                                          #  1000 ->  ~2100 
print("tonto" %in% posts_freq_words_train)    
print("imbecil" %in% posts_freq_words_train)  # <- FALSE with freq 1000
print("cabron" %in% posts_freq_words_train)   # <- FALSE with freq  150
```

Vamos a probar primero usando los términos que aparecen al menos 1000 veces.


Y ahora utilizamos ese listado para limitar el número de columnas/features tanto del conjuntos de entrenamiento como del de test:

```{r}
dim(posts_dtm_train)
posts_dtm_freq_train <- posts_dtm_train[ , posts_freq_words_train]
dim(posts_dtm_freq_train)

dim(posts_dtm_test)
posts_dtm_freq_test <- posts_dtm_test[ , posts_freq_words_train]     
dim(posts_dtm_freq_test)
```




## Paso 3: Entrenamiento del modelo


Se ha intentado utilizar la librería caret para obtener los mejores hipérparametros posibles.

No ha sido posible. Con este dataset, los tiempos eran demasiado grandes.

(caret usa la librería kernlab, que parece ser que es muy lenta).


Se prueba entonces con LibSVM (e1071), y con LiblinearR:


### LIBSVM (e1071)


NO Probamos primero con LIBSVM (e1071). Es muy ineficiente.


```{r}
#system.time({
#  train the model e1071
#    svm_e1071 <- svm(x = posts_dtm_freq_train, y = posts_train_labels, scale = TRUE)     # default values:  kernel = RBF, cost = 1
#    svm_e1071 <- svm(x = posts_freq_train_df, y = posts_train_labels, kernel = "linear", scale = TRUE)  # default values:  kernel = RBF, cost = 1  <- it takes a lot ...
#})
  
# svm_e1071
```

Se tarda muchísimo.
RBF -> más de una hora corriendo, y no ha terminado ... 
Lineal -> más de 5 min., y tampoco (se tardan segundos con la librería LiblinearR)


Se descarta también intentar mejorar el modelo LIBSVM con tune. No tiene sentido. 
Se ha visto que para este tipo de problema, LIBSVM no es muy bueno.


LiblineaR es muchísimo más eficiente.


### LiblineaR


```{r}
chunk_size <- 10000   

system.time({
  chunk_list_train <- creat_mat_in_chunks(posts_dtm_freq_train, chunk_size)
  
  posts_freq_train_mat_chunks <- do.call(rbind, chunk_list_train)
  
  rm(chunk_list_train)
})
```


```{r}
system.time({
  chunk_list_test <- creat_mat_in_chunks(posts_dtm_freq_test, chunk_size)
  
  posts_freq_test_mat_chunks <- do.call(rbind, chunk_list_test)
  
  rm(chunk_list_test)
})
```



```{r}
# dtm -> matrix

#posts_freq_train_mat <- as.matrix(posts_dtm_freq_train)
#posts_freq_train_df <- as.data.frame(posts_freq_train_mat)

#posts_freq_test_mat <- as.matrix(posts_dtm_freq_test)
#posts_freq_test_df <- as.data.frame(posts_freq_test_mat)
```


```{r}
posts_freq_train_mat <- posts_freq_train_mat_chunks
rm(posts_freq_train_mat_chunks)

posts_freq_test_mat <- posts_freq_test_mat_chunks
rm(posts_freq_test_mat_chunks)
```


Entrenamiento (coste = 1)

```{r}
system.time({

  liblinear_svm_model <- LiblineaR(data = posts_freq_train_mat, target = posts_train_labels, type = 3)  # cost = 1

})
```


Este modelo tarda 14-15 s en entrenar con el conjunto 100k !!!  (type = 1)(dual)  
Este modelo tarda 4 s en entrenar con el conjunto 100k !!!  (type = 2)  (primal) 
Este modelo tarda 5-7 s en entrenar con el conjunto 100k !!!  (type = 3) (dual)
Este modelo tarda 4 s en entrenar con el conjunto 100k !!! (type = 5) (primal?)


"We recommend users

1. Try the default dual-based solver first.
2. If it is slow, check primal-based solvers.

To choose between using L1 and L2 regularization, we recommend trying L2 first unless
users need a sparse model."


```{r}
# prediction
system.time({
  
  prediction_liblinear <- predict(liblinear_svm_model, posts_freq_test_mat)

})
```

```{r}
table(as.factor(prediction_liblinear$predictions))
```



```{r}
# confusion matrix
confusionMatrix(reference = as.factor(posts_test_labels), data = as.factor(prediction_liblinear$predictions), positive="1", mode = "everything")
```

RESULTADOS FINALES:

- Con el sub dataset de 300k:

freq baja (100,200,300) -> sesión R "explota"
freq intermedia (400) -> sesión R explota
freq intermedia (500) -> alta exactitud (96%), kappa baja (0.28)
                         Pesos 1/2 -> 96% y 0.36
                         Pesos 1/5 -> 95% y 0.38
freq alta (1000) -> resultados absurdos (0 TP, kappa 0, y los pesos no lo arreglan)


- Con el sub dataset de 100k:
freq baja (100) -> exactitud alta (92.5%) y kappa aceptable (0.58)
                   **Con pesos 1/2, la exactitud se mantiene (92%), kappa 0.61**

- Con el sub dataset de 30k (que está muy balanceado: 19k no odio vs 11k odio):

freq alta (500)  -> baja exactitud, kappa horrible
freq baja (100)  -> alta exactitud (83%), muy buen kappa (0.62)



Confusion Matrix and Statistics

          Reference
Prediction     0     1
         0 71430  2235
         1   368   531
                                          
               Accuracy : 0.9651          
                 95% CI : (0.9637, 0.9664)
    No Information Rate : 0.9629          
    P-Value [Acc > NIR] : 0.0007463       
                                          
                  Kappa : 0.2766          
                                          
 Mcnemar's Test P-Value : < 2.2e-16       
                                          
            Sensitivity : 0.191974        
            Specificity : 0.994875        
         Pos Pred Value : 0.590656        
         Neg Pred Value : 0.969660        
              Precision : 0.590656        
                 Recall : 0.191974        
                     F1 : 0.289768        
             Prevalence : 0.037096        
         Detection Rate : 0.007121        
   Detection Prevalence : 0.012057        
      Balanced Accuracy : 0.593424        
                                          
       'Positive' Class : 1 



Resultados:

Confusion Matrix and Statistics

          Reference
Prediction     0     1
         0 12004  1035
         1   496  1731
                                          
               Accuracy : 0.8997          
                 95% CI : (0.8948, 0.9044)
    No Information Rate : 0.8188          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.6343          
                                          
 Mcnemar's Test P-Value : < 2.2e-16       
                                          
            Sensitivity : 0.6258          
            Specificity : 0.9603          
         Pos Pred Value : 0.7773          
         Neg Pred Value : 0.9206          
              Precision : 0.7773          
                 Recall : 0.6258          
                     F1 : 0.6934          
             Prevalence : 0.1812          
         Detection Rate : 0.1134          
   Detection Prevalence : 0.1459          
      Balanced Accuracy : 0.7931          
                                          
       'Positive' Class : 1  

   
Se obtienen una exactitud y un kappa bastante buenos.

   
- Intentamos mejorar el modelo usando el coste calculado por  heuristicC:   
      

```{r}
c <- heuristicC(posts_freq_train_mat)

c
```

c <-  0.8473049  con el dataset completo

c <- 0.8719082   con el dataset 200k

c <- 0.1970407 con el dataset 100k ????

```{r}
system.time({
  liblinear_svm_model_c <- LiblineaR(data = posts_freq_train_mat, target = posts_train_labels, type = 3, cost = c)  
})
```

```{r}
# prediction
system.time({
  prediction_liblinear_c <- predict(liblinear_svm_model_c, posts_freq_test_mat)
})
```

```{r}
# confusion matrix
confusionMatrix(reference = as.factor(posts_test_labels), data = as.factor(prediction_liblinear_c$predictions), positive="1", mode = "everything")
```

Ni usando el coste calculado por la función heurística, ni usando unos costes mayores (c+1, c+10), se mejora el resultado con coste = 1.


- Intentamos mejorar el modelo usando pesos, para favorecer la clase minoritaria:


```{r}
# Define class weights
class_weights <- c("0" = 1, "1" = 2)  # Assign higher weight to the minority class
```



```{r}
system.time({

  liblinear_svm_model_weights <- LiblineaR(data = posts_freq_train_mat, target = posts_train_labels, 
                                           type = 3,
                                           wi = class_weights)
})
```


```{r}
# prediction
system.time({
  
  prediction_liblinear_weights <- predict(liblinear_svm_model_weights, posts_freq_test_mat)

})
```

```{r}
#Confusion matrix
confusionMatrix(reference = as.factor(posts_test_labels), 
                data = as.factor(prediction_liblinear_weights$predictions), 
                positive="1", 
                mode = "everything")
```

Guardamos el modelo:

```{r}
#save model
saveRDS(liblinear_svm_model_weights, file="svm_liblinear_100k_freq100_weights1_2.rds")

# Load the model
#loaded_model <- readRDS("decision_tree_model.rds")
```


kappa:   0.21 - 0.4 <- fair agreement
         0.41 - 0.6 <- modereate agreement
         0.61 - 0.8 <- substantial/good agreement


- Resultados con freq = 500

Con pesos 1/10:

Confusion Matrix and Statistics

          Reference
Prediction     0     1
         0 68499  1343
         1  3299  1423
                                         
               Accuracy : 0.9377         
                 95% CI : (0.936, 0.9395)
    No Information Rate : 0.9629         
    P-Value [Acc > NIR] : 1              
                                         
                  Kappa : 0.3496  
  
Con pesos 1/5:  
  
Confusion Matrix and Statistics

          Reference
Prediction     0     1
         0 69721  1520
         1  2072  1246
                                          
               Accuracy : 0.9518          
                 95% CI : (0.9503, 0.9533)
    No Information Rate : 0.9629          
    P-Value [Acc > NIR] : 1               
                                          
                  Kappa : 0.3847          
                                          
 Mcnemar's Test P-Value : <2e-16          
                                          
            Sensitivity : 0.45047         
            Specificity : 0.97114         
         Pos Pred Value : 0.37553         
         Neg Pred Value : 0.97866         
              Precision : 0.37553         
                 Recall : 0.45047         
                     F1 : 0.40960         
             Prevalence : 0.03710         
         Detection Rate : 0.01671         
   Detection Prevalence : 0.04450         
      Balanced Accuracy : 0.71080         
                                          
       'Positive' Class : 1          
       
       
  
- Resultados con dataset 100k, freq = 100 , pesos 1/2

Accuracy : 0.9225 y kappa 0.61 !!!


Confusion Matrix and Statistics

          Reference
Prediction     0     1
         0 21112   945
         1   981  1821
                                          
               Accuracy : 0.9225          
                 95% CI : (0.9191, 0.9258)
    No Information Rate : 0.8887          
    P-Value [Acc > NIR] : <2e-16          
                                          
                  Kappa : 0.6105          
                                          
 Mcnemar's Test P-Value : 0.4252          
                                          
            Sensitivity : 0.65835         
            Specificity : 0.95560         
         Pos Pred Value : 0.64989         
         Neg Pred Value : 0.95716         
              Precision : 0.64989         
                 Recall : 0.65835         
                     F1 : 0.65409         
             Prevalence : 0.11127         
         Detection Rate : 0.07325         
   Detection Prevalence : 0.11272         
      Balanced Accuracy : 0.80697         
                                          
       'Positive' Class : 1    
  
                  
                  
                  
                  
El mejor kappa (0.635, 0.6269) se obtiene con la relacción 2/5


El mejor modelo es el tipo 3 (L2-regularized L1-loss support vector classification (dual)), y con un balance de pesos 1 a 3 para la clase positiva (la clase minoritaria, los mensajes de odio en este caso).
Con este modelo la exactitud apenas se ve afectada, pero el kappa mejora ligeramente.




----------------------------------------------------------------------------------





freq = 100:

Confusion Matrix and Statistics

          Reference
Prediction     0     1
         0 14127   840
         1   873  1926
                                          
               Accuracy : 0.9036          
                 95% CI : (0.8991, 0.9079)
    No Information Rate : 0.8443          
    P-Value [Acc > NIR] : <2e-16          
                                          
                  Kappa : 0.635  



Probar con otra freq

Probar con librería quateda:

https://tutorials.quanteda.io/basic-operations/workflow/


