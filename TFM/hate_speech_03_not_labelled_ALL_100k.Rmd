---
title: "hate_speech_03_not_labelled_ALL_100k"
author: "Fran Camacho"
date: "2025-07-24"
output: word_document
params:
  complete: FALSE 
  crossValidation: FALSE
  gridSearch: TRUE
---


# TFM - Clasificación de comentarios no etiquetados - (3) Mejora de datasets - 100k


Se va a intentar mejorar el resultado obtenido al clasificar comentarios de lectores no etiquetados habiendo entrenado SVM 
de la librería LiblineaR solo con los datasets encontrados en internet (Hatemedia, HuggingFace y Kaggle).

Para ello añadiremos los comentarios de los lectores de El Mundo al dataset total de 100k observaciones.


Variables que configuran la manera en la que se ejecuta el proceso:

```{r}
complete_dataset <- params$complete 
crossValidation <- params$crossValidation
gridSearch <- params$gridSearch
  
#if no complete dataset, number of no hate messages to pick from the total dataset
size <- 78000     # 78000 77432 number of random rows of no hate messages 

# number of min. freq (the lower the size, the lower the freq)
freq <- 20 

# Assign higher weight to the minority class
class_weights <- c("0" = 1, "1" = 2)  

random_seed <- 123
```

```{r}
print(getwd())
```


## 1 - Leer los CSVs


```{r}
source('hate_speech_03_common.R')
```


```{r}
load_libraries()
```


Importamos los datasets:

```{r}
# import the CSV files

odio_hatemedia_raw <- read.csv(file.path("../TFM/dataset_01_hatemedia.csv"), sep=";")  
odio_hatemedia_raw <- odio_hatemedia_raw[-1]     # -> 574272 obs. of 2 variables

odio_huggingface_raw <- read.csv(file.path("../TFM/dataset_02_huggingface.csv"), sep=";")  
odio_huggingface_raw <- odio_huggingface_raw[-1]     # -> ~29855 obs. of 2 variables

odio_kaggle_raw <- read.csv(file.path("../TFM/dataset_03_kaggle.csv"), sep=";") 
odio_kaggle_raw <- odio_kaggle_raw[-1]             # -> ~11180 obs. (only spanish) of 2 variables

odio_kaggle_test_raw <- read.csv(file.path("../TFM/dataset_04_kaggle.csv"), sep=";") 
odio_kaggle_test_raw <- odio_kaggle_test_raw[-1]    # -> 1243 obs. (only spanish) of 2 variables
```


El dataset de hatemedia es algo diferente. Procedemos a eliminar las "," para evitar problemas.

```{r}
# Replace all "," in this dataset. If not, after processing it we get lines of only one "huge word"
odio_hatemedia_raw$post <- gsub(',',' ',odio_hatemedia_raw$post)
```


Ahora ya se puede crear un dataset con todos los datasets disponibles:

```{r}
hate_raw <- rbind(odio_hatemedia_raw, odio_huggingface_raw, odio_kaggle_raw, odio_kaggle_test_raw)

dim(hate_raw)
```


Con el dataset de Hatemedia, se obtuvieron muy buenos resultados cuando se seleccionarion 100k observaciones.
Vamos a realizar el mismo proceso aquí: del total de observaciones, vamos a seleccionar todos los mensajes de odio,
y añadir mensajes no de odio hasta llegar a las 100k observaciones (aproximadamente, al realizar el preprocesado, 
seguramente algunas sean eliminadas por estar vacías).

Como disponemos de 22568 mensajes de odio, vamos a seleccionar de manera aleatoria el siguiente número de mensajes de no odio:

```{r}
no_hate_n <- 100000 - 22568
cat("Number of no hate messages to be selected: ",no_hate_n,"\n")
```



Preparamos el dataset de 100k observaciones:

```{r}
set.seed(random_seed)  # idea: repeat the process with different seeds,
                       # to see the influence of randomly chosen rows <- no influence seen

# Prepare a bit smaller dataset:  100k
if (!complete_dataset) {
  print("Preparing a smaller dataset")

  df_hate <- hate_raw[hate_raw$label == 1, ]
  df_no_hate <- hate_raw[hate_raw$label == 0, ]

  n  <- nrow(df_no_hate)
  k  <- size  # number of random rows with no hate messages

  ids <- sample(n,size = k, replace = FALSE)
  df_no_hate_sample <- df_no_hate[ids, ,drop = FALSE]

  # join sample dataset with no hate obs. with hate obs.
  hate_raw_sample_k <- rbind(df_no_hate_sample, df_hate)

  #so we don't have to change variable names
  hate_raw <- hate_raw_sample_k
  
  rm(df_hate,df_no_hate,df_no_hate_sample,hate_raw_sample_k)
  
} else {
  print("Working with the whooole dataset. Be patient!!!")
}
```

Dim(hate_raw):

```{r}
dim(hate_raw)
```



Comprobar proporción de los mensajes en este dataset después del "downsampling":

```{r}
table(hate_raw$label)
prop.table(table(hate_raw$label))
```


Ahora añadimos los 2500 mensajes de El Mundo obtenidos en junio 2025:

```{r}
subset1_labelled <- read.csv("subset1_labelled.csv", sep = "|", header = TRUE)
subset2_labelled <- read.csv("subset2_labelled.csv", sep = "|", header = TRUE)
subset3_labelled <- read.csv("subset3_labelled.csv", sep = "|", header = TRUE) # comment this line to test with 2000 posts
```



Ahora ya se puede crear un dataset TOTAL con todos los datasets disponibles:

```{r}
#hate_raw <- rbind(subset1_labelled, hate_raw, subset2_labelled) # uncomment to test with 2000 posts
hate_raw <- rbind(subset1_labelled, hate_raw, subset2_labelled, subset3_labelled)
dim(hate_raw)
```


También leemos los comentarios de test igualmente ya etiquetados:

```{r}
posts_test_raw <- read.csv("posts_test_labelled.csv", sep = "|", header = TRUE)
```


Comprobar proporción de los mensajes en estos datasets:

-dataset entrenamiento:

```{r}
table(hate_raw$label)
prop.table(table(hate_raw$label))
```


Añadir estos 2000 (2500) comentarios nuevos no cambia apenas la proporción ya existente en el dataset de entrenamiento.

-dataset test:

```{r}
table(posts_test_raw$label)
prop.table(table(posts_test_raw$label))
```



## 2 - Preparar los datasets

La columna "label" es de tipo int. Ya que se trata en realidad de una variable categórica 0/1, es conveniente transformarla en un factor:

```{r}
#Convert class into a factor
hate_raw$label <- factor(hate_raw$label)

# also test dataset
posts_test_raw$label <- factor(posts_test_raw$label)
```


```{r}
#check_text(hate_raw$post)
system.time({
  hate <- preprocess_posts(hate, hate_raw)
  
  # additional cleaning for posts from El Mundo
  hate$post <- gsub("#\\d+ Cerrar", "", hate$post) 
  hate$post <- gsub("# \\d+", "", hate$post) 
})

```

```{r}
#save hate object
#save(hate, file = "hate_100k_df.RData")
```


También el dataset de test:

```{r}
# Also the test dataset
system.time({
  posts_test <- preprocess_posts(posts_test, posts_test_raw)
  
  # additional cleaning for posts from El Mundo
  posts_test$post <- gsub("#\\d+ Cerrar", "", posts_test$post) 
  posts_test$post <- gsub("# \\d+", "", posts_test$post) 
})
```


```{r}
#save posts_test object
#save(posts_test, file = "posts_test_df.RData")
```



```{r}
# Number of rows deleted:
obs_removed <- nrow(hate_raw)-nrow(hate)

cat("Se han eliminado ", obs_removed, " líneas al procesar el dataset.")

rm(hate_raw)
```



**Corpus de los textos**

Ya se puede proceder a la creación del objeto corpus con todas los mensajes:

```{r}
#create corpus
system.time({
  posts_corpus <- VCorpus(VectorSource(hate$post))
})

print(posts_corpus)
```

También para el dataset de test:

```{r}
#create corpus for test dataset
system.time({
  posts_corpus_test <- VCorpus(VectorSource(posts_test$post))
})

print(posts_corpus_test)
```


Limpieza de los textos del corpus:

Procesado habitual: eliminar mayúsculas, números ...

```{r}
system.time({
  posts_corpus_clean <- clean_corpus(posts_corpus)
})
```

```{r}
#save hate object
#save(posts_corpus_clean, file = "posts_corpus_clean.RData")
```

También para el conjunto de test:

```{r}
system.time({
  posts_corpus_test_clean <- clean_corpus(posts_corpus_test)
})
```


Finalmente, se procede a la **"tokenización"** de los comentarios:

```{r}
system.time({
  posts_dtm_train <- DocumentTermMatrix(posts_corpus_clean)
  posts_dtm_train
})
```

[

En este punto eliminar los objetos "corpus". Ya no son necesarios.

```{r}
rm(posts_corpus)
rm(posts_corpus_clean)
```
  
]


Se necesita ahora obtener el listado de las palabras más utilizadas:


```{r}
#freq <- 10

# Data preparation – creating indicator features for frequent words
# the function findFreqTerms() in the tm package takes a DTM and returns a character vector
# containing words that appear at least a  minimum number of times
posts_freq_words_train <- findFreqTerms(posts_dtm_train, freq)  #   10 -> 20139 terms  (before adding the last dataset -El Mundo-, it was ~17000) 26296 terms (2500 posts)
                                                                #   20 -> 16728 terms (2000 posts) 16776 terms (2500 posts)

```


Para crear la DTM del conjunto de test, se necesita el mismo vocabulario del conjunto de entrenamiento:

```{r}
train_vocab <- posts_freq_words_train

posts_dtm_freq_test <- DocumentTermMatrix(posts_corpus_test_clean,
                                     control = list(dictionary = train_vocab,
                                               wordLengths = c(3, Inf))
                                    )

posts_dtm_freq_test
```


Y ahora utilizamos ese listado para limitar el número de columnas/features tanto del conjuntos de entrenamiento como del de test:
(El de test ya ha sido creado con el número correcto de columnas/features)

```{r}
dim(posts_dtm_train)
posts_dtm_freq_train <- posts_dtm_train[ , posts_freq_words_train]
dim(posts_dtm_freq_train)

#dim(posts_dtm_test)
#posts_dtm_freq_test <- posts_dtm_test[ , posts_freq_words_train]     
dim(posts_dtm_freq_test)
```



## Paso 3: Entrenamiento del modelo LiblineaR


Necesitamos convertir las DTMs en matrices para poder entrenar el modelo.
Dado el tamaño del dataset,y las limitaciones físicas del equipo en el que se realiza este proceso,
se procesa en lotes las DTMs, y posteriormente se juntan las matrices para obtener la total.


```{r}
chunk_size <- 10000   

system.time({
  chunk_list_train <- creat_sparse_mat_in_chunks(posts_dtm_freq_train, chunk_size)
  
  posts_freq_train_mat_chunks <- do.call(rbind, chunk_list_train)
  
  rm(chunk_list_train)
})
```



```{r}
system.time({
  chunk_list_test <- creat_sparse_mat_in_chunks(posts_dtm_freq_test, chunk_size)
  
  posts_freq_test_mat_chunks <- do.call(rbind, chunk_list_test)
  
  rm(chunk_list_test)
})
```



```{r}
posts_freq_train_mat <- posts_freq_train_mat_chunks
rm(posts_freq_train_mat_chunks)

posts_freq_test_mat <- posts_freq_test_mat_chunks
rm(posts_freq_test_mat_chunks)
```



Entrenamiento (coste = 1)


```{r}
system.time({
  liblinear_svm_model <- LiblineaR(data = posts_freq_train_mat, target = hate$label, type = 3)  # cost = 1
})

#liblinear_svm_model
```


Predicción:

```{r}
# prediction
system.time({
  prediction_liblinear <- predict(liblinear_svm_model, posts_freq_test_mat)
})
```



```{r}
table(as.factor(prediction_liblinear$predictions))
```


Evaluación del resultado:

```{r}
# confusion matrix
confusionMatrix(reference = as.factor(posts_test$label), data = as.factor(prediction_liblinear$predictions), positive="1", mode = "everything")
```

**Habiendo añadido 2000 comentarios (subset1 y subset2)**

Confusion Matrix and Statistics

          Reference
Prediction   0   1
         0 410  26
         1  43  21
                                         
Accuracy : 0.862          
   Kappa : 0.3028         
      F1 : 0.3784         
       
       
       
**Habiendo añadido 2500 comentarios (subset1, subset2 y subset3)**

          Reference
Prediction   0   1
         0 425  27
         1  28  20
                                          
Accuracy : 0.89            
   Kappa : 0.3603          
      F1 : 0.4211          


- size = 78000 (total 103k):

proporción:
       0        1 
0.777933 0.222067 

Accuracy : 0.906
   Kappa : 0.3963

- size = 48000 (total 73k):

proporción:
        0         1 
0.6867575 0.3132425 
 
 Accuracy : 0.89 
    Kappa : 0.4271    

- size= 38000 (total 63k):

proporción:
        0         1 
0.6370901 0.3629099 

 Accuracy : 0.91
    Kappa : 0.4338 


- size= 28000 (total 53k):

proporción:
        0         1 
0.5687043 0.4312957 

 Accuracy : 0.906
    Kappa : 0.4428 
    

- Búsqueda en rejilla:


```{r}
# Find the best model combining type, cost, bias, and weights
#
system.time({
  
  if (gridSearch) {
  
    tryTypes <- c(1,3,5)  # type 2 gives worst results       
    tryCosts <- c(1,10)
    tryBias <- c(1,10)
    tryWeights <- list(c(1,2),c(1,3),c(1,5)) #,c(1,10))
 
    grid_search_result <- grid_search(posts_freq_train_mat, hate$label, posts_test$label,
                                  tryTypes, tryCosts, tryBias, tryWeights)   
  }
  
})
```

**Habiendo añadido 2000 comentarios (subset1 y subset2)**

[1] "Doing grid search"
Results for type = 3
Results for C = 0.1 bias = -1 weights = 12: 0.75 accuracy, 0.2220563 kappa.
Results for C = 0.1 bias = -1 weights = 13: 0.734 accuracy, 0.2120853 kappa.
Results for C = 0.1 bias = -1 weights = 15: 0.694 accuracy, 0.1803454 kappa.
Results for C = 0.1 bias = -1 weights = 110: 0.664 accuracy, 0.1686954 kappa.
Results for C = 0.1 bias = 1 weights = 12: 0.826 accuracy, 0.2655501 kappa.
Results for C = 0.1 bias = 1 weights = 13: 0.802 accuracy, 0.2468505 kappa.
Results for C = 0.1 bias = 1 weights = 15: 0.768 accuracy, 0.2355748 kappa.
Results for C = 0.1 bias = 1 weights = 110: 0.718 accuracy, 0.2102787 kappa.
Results for C = 0.1 bias = 10 weights = 12: 0.826 accuracy, 0.2655501 kappa.
Results for C = 0.1 bias = 10 weights = 13: 0.8 accuracy, 0.2439135 kappa.
Results for C = 0.1 bias = 10 weights = 15: 0.768 accuracy, 0.2355748 kappa.
Results for C = 0.1 bias = 10 weights = 110: 0.718 accuracy, 0.2102787 kappa.
Results for C = 1 bias = -1 weights = 12: 0.77 accuracy, 0.2380877 kappa.
Results for C = 1 bias = -1 weights = 13: 0.762 accuracy, 0.2362395 kappa.
Results for C = 1 bias = -1 weights = 15: 0.73 accuracy, 0.1924678 kappa.
Results for C = 1 bias = -1 weights = 110: 0.718 accuracy, 0.1956462 kappa.
Results for C = 1 bias = 1 weights = 12: 0.822 accuracy, 0.2783472 kappa.
Results for C = 1 bias = 1 weights = 13: 0.796 accuracy, 0.2472769 kappa.
Results for C = 1 bias = 1 weights = 15: 0.794 accuracy, 0.2533202 kappa.
Results for C = 1 bias = 1 weights = 110: 0.746 accuracy, 0.2015391 kappa.
**Results for C = 1 bias = 10 weights = 12: 0.824 accuracy, 0.2817265 kappa.**
Results for C = 1 bias = 10 weights = 13: 0.798 accuracy, 0.2501633 kappa.
Results for C = 1 bias = 10 weights = 15: 0.786 accuracy, 0.2507842 kappa.
Results for C = 1 bias = 10 weights = 110: 0.762 accuracy, 0.219979 kappa.
Results for C = 10 bias = -1 weights = 12: 0.758 accuracy, 0.2314142 kappa.
Results for C = 10 bias = -1 weights = 13: 0.75 accuracy, 0.2298403 kappa.
Results for C = 10 bias = -1 weights = 15: 0.73 accuracy, 0.207858 kappa.
Results for C = 10 bias = -1 weights = 110: 0.728 accuracy, 0.1904184 kappa.
Results for C = 10 bias = 1 weights = 12: 0.806 accuracy, 0.2237019 kappa.
Results for C = 10 bias = 1 weights = 13: 0.814 accuracy, 0.2652636 kappa.
Results for C = 10 bias = 1 weights = 15: 0.782 accuracy, 0.2098243 kappa.
Results for C = 10 bias = 1 weights = 110: 0.756 accuracy, 0.1960355 kappa.
Results for C = 10 bias = 10 weights = 12: 0.788 accuracy, 0.2449067 kappa.
Results for C = 10 bias = 10 weights = 13: 0.802 accuracy, 0.2560418 kappa.
Results for C = 10 bias = 10 weights = 15: 0.674 accuracy, 0.1835958 kappa.
Results for C = 10 bias = 10 weights = 110: 0.804 accuracy, 0.2590352 kappa.
   user  system elapsed 
245.886   0.386 246.268 

**Habiendo añadido 2500 comentarios (subset1, subset2 y subset3)**

[1] "Doing grid search"
Results for type = 3
Results for C = 0.1 bias = -1 weights = 12: 0.8 accuracy, 0.2793104 kappa.
Results for C = 0.1 bias = -1 weights = 13: 0.786 accuracy, 0.2910903 kappa.
Results for C = 0.1 bias = -1 weights = 15: 0.746 accuracy, 0.2403216 kappa.
Results for C = 0.1 bias = -1 weights = 110: 0.708 accuracy, 0.2344959 kappa.
Results for C = 0.1 bias = 1 weights = 12: 0.878 accuracy, 0.3627246 kappa.
Results for C = 0.1 bias = 1 weights = 13: 0.842 accuracy, 0.3238154 kappa.
Results for C = 0.1 bias = 1 weights = 15: 0.798 accuracy, 0.2927963 kappa.
Results for C = 0.1 bias = 1 weights = 110: 0.764 accuracy, 0.2837198 kappa.
Results for C = 0.1 bias = 10 weights = 12: 0.878 accuracy, 0.3627246 kappa.
Results for C = 0.1 bias = 10 weights = 13: 0.848 accuracy, 0.3449405 kappa.
Results for C = 0.1 bias = 10 weights = 15: 0.796 accuracy, 0.2898321 kappa.
Results for C = 0.1 bias = 10 weights = 110: 0.764 accuracy, 0.2837198 kappa.
Results for C = 1 bias = -1 weights = 12: 0.824 accuracy, 0.3178929 kappa.
Results for C = 1 bias = -1 weights = 13: 0.798 accuracy, 0.3007477 kappa.
Results for C = 1 bias = -1 weights = 15: 0.796 accuracy, 0.3131313 kappa.
Results for C = 1 bias = -1 weights = 110: 0.764 accuracy, 0.2837198 kappa.
Results for C = 1 bias = 1 weights = 12: 0.86 accuracy, 0.3608707 kappa.
Results for C = 1 bias = 1 weights = 13: 0.858 accuracy, 0.3837234 kappa.
Results for C = 1 bias = 1 weights = 15: 0.836 accuracy, 0.3479851 kappa.
Results for C = 1 bias = 1 weights = 110: 0.796 accuracy, 0.3131313 kappa.
Results for C = 1 bias = 10 weights = 12: 0.864 accuracy, 0.3791315 kappa.
**Results for C = 1 bias = 10 weights = 13: 0.858 accuracy, 0.3922898 kappa.**
Results for C = 1 bias = 10 weights = 15: 0.84 accuracy, 0.3719974 kappa.
Results for C = 1 bias = 10 weights = 110: 0.798 accuracy, 0.3161259 kappa.
Results for C = 10 bias = -1 weights = 12: 0.79 accuracy, 0.2967745 kappa.
Results for C = 10 bias = -1 weights = 13: 0.796 accuracy, 0.2898321 kappa.
Results for C = 10 bias = -1 weights = 15: 0.782 accuracy, 0.285527 kappa.
Results for C = 10 bias = -1 weights = 110: 0.77 accuracy, 0.2695259 kappa.
Results for C = 10 bias = 1 weights = 12: 0.856 accuracy, 0.3615436 kappa.
Results for C = 10 bias = 1 weights = 13: 0.852 accuracy, 0.3531242 kappa.
Results for C = 10 bias = 1 weights = 15: 0.824 accuracy, 0.3263725 kappa.
Results for C = 10 bias = 1 weights = 110: 0.81 accuracy, 0.3031103 kappa.
Results for C = 10 bias = 10 weights = 12: 0.834 accuracy, 0.3442675 kappa.
Results for C = 10 bias = 10 weights = 13: 0.864 accuracy, 0.3882031 kappa.
Results for C = 10 bias = 10 weights = 15: 0.91 accuracy, 0.3393235 kappa.
Results for C = 10 bias = 10 weights = 110: 0.826 accuracy, 0.3541011 kappa.
   user  system elapsed 
253.404   0.377 253.899 




```{r}
if (gridSearch) {
  print(grid_search_result)
}
```

**Habiendo añadido 2000 comentarios (subset1 y subset2)**

[1] "gridSearch result: "
Best model type is: 3 
Best cost is: 1 
Best bias is: 10 
Best weights are: 1 2 
Best accuracy is: 0.824
Best kappa is: 0.281726 


Cambiando los pesos, incluso realizando muchas combinaciones con diferentes parámetros como las que se muestran, 
**no** se obtiene un resultado mejor que con los valores por defecto.


En cuanto al resultado con los valores por defecto:

          Reference
Prediction   0   1
         0 410  26
         1  43  21
                                         
Accuracy : 0.862          
   Kappa : 0.3028         
      F1 : 0.3784 


comparado con el dataset total de 100k observaciones sin los mensajes nuevos, es muchísimo mejor:

          Reference
Prediction   0   1
         0 197   7
         1 681 115
                                          
Accuracy : 0.312           
   Kappa : 0.0494          
      F1 : 0.2505 
                     
                     
Esto me hace pensar que el resultado sería mejor todavía añadiendo más mensajes nuevos clasificados a ese dataset (de 100k obs.)


**Habiendo añadido 2500 comentarios (subset1, subset2 y subset3)**


[1] "gridSearch result: "
Best model type is: 3 
Best cost is: 1 
Best bias is: 10 
Best weights are: 1 3 
Best accuracy is: 0.858 
**Best kappa is: 0.3922898 **



Con 2500 comentarios añadidos al dataset "total" se empieza a apreciar una mejora apreciable en kappa.


[1] "gridSearch result: "
Best model type is: 2 
Best cost is: 10 
Best bias is: 1 
Best weights are: 1 2 
**Best accuracy is: 0.896**
**Best kappa is: 0.4613185**

100k -freq = 50

[1] "gridSearch result: "
Best model type is: 1 
Best cost is: 1
Best bias is: 10 
Best weights are: 1 2 
**Best accuracy is: 0.894**
**Best kappa is: 0.4731**


----------------------------------------------------------------------------------------------------

Otros intentos anteriores (todos con size = 78k para tener en total ~100k observaciones/comentarios)

```{r}

bestType <- 3 
bestCost <- 1 
bestBias <- 10 
bests_weights <- c("0" = 1, "1" = 3) 

#training
system.time({
  liblinear_svm_model_best <- LiblineaR(data = posts_freq_train_mat, target = hate$label, 
                                        type = bestType,
                                        cost = bestCost,
                                        bias = bestBias,
                                        weights = bests_weights)
})

# prediction
system.time({
  prediction_liblinear_best <- predict(liblinear_svm_model_best, posts_freq_test_mat)
})

# confusion matrix
confusionMatrix(reference = as.factor(posts_test$label), data = as.factor(prediction_liblinear_best$predictions), positive="1", mode = "everything")
```

```{r}

bestType <- 2 
bestCost <- 10 
bestBias <- 1 
bests_weights <- c("0" = 1, "1" = 2) 

#training
system.time({
  liblinear_svm_model_best <- LiblineaR(data = posts_freq_train_mat, target = hate$label, 
                                        type = bestType,
                                        cost = bestCost,
                                        bias = bestBias,
                                        weights = bests_weights)
})

# prediction
system.time({
  prediction_liblinear_best <- predict(liblinear_svm_model_best, posts_freq_test_mat)
})

# confusion matrix
confusionMatrix(reference = as.factor(posts_test$label), data = as.factor(prediction_liblinear_best$predictions), positive="1", mode = "everything")
```



