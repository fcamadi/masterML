---
title: "Tema7_Ejercicio_redes_neuronas"
author: "Fran Camacho"
date: "2025-02-24"
output:
  pdf_document: default
  github_document: default
  word_document: default
params:
  library: keras  #options: keras, neuralnet, RSNNS
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library <- params$library 
print(library)
```

# Tema 7 - Ejercicio

La base de datos incluida en el archivo Bank.csv (dentro de Bank.zip)
recoge información de 4.521 clientes a los que se les ofreció contratar un
depósito a plazo en una entidad bancaria portuguesa (el zip también
contiene un fichero de texto denominado Bank-names.txt con el detalle
completo de todas las variables incluidas)
Utilizando dicha base de datos, elabore una red neuronal que permita
pronosticar si, en base a sus características, el cliente contratará el
depósito o no.

De cara a la realización de este ejercicio, debe tener en cuenta que:

- La variable objetivo de nuestro modelo es “y”, la cual tiene el valor
“yes” si el cliente ha contratado el depósito y “no” en caso contrario.
- Observe que hay múltiples variable de tipo cualitativo que deberá
transformar antes de estimar el modelo.
- No olvide normalizar los datos antes de introducirlos en el modelo.
- Recuerde especificar el número de capas ocultas y neuronas
utilizadas, así como el umbral de error permitido y el algoritmo de
cálculo elegidos. Se permite realizar y presentar variaciones del
modelo a fin de obtener un ajuste óptimo.
- Deberá dejar un porcentaje del dataset para validar los resultados de
la red neuronal estimada.


## Paso 1: Carga de los datos

```{r}
# import the CSV file
bank_raw <- read.csv(file.path("Chapter07/Bank", "bank.csv"), sep = ";", stringsAsFactors = TRUE)
```


## Paso 2: Explorar y preparar los datos

Carga de paquetes que son necesarios para diversas funciones.

```{r, echo = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = FALSE)

if (library=="neuralnet") {
  print("Choosing neuralnet")
  
  if (!require(neuralnet)) install.packages('neuralnet', dependencies = T)
  library(neuralnet)
  
} else if (library=="RSNNS") {
  print("Choosing RSNNS")
  
  # Downloading packages -------------------------------------------------------
  if (!require(RSNNS)) install.packages('RSNNS', dependencies = T)
  library(RSNNS)
  
} else {
  print("Choosing Keras")
  
  if (!require(keras3)) install.packages('keras3', dependencies = T)
  library(keras3)
  #install_keras()
  
  if (!require(tidyverse)) install.packages('tidyverse', dependencies = T)
  library(tidyverse)
  
  if (!require(jsonlite)) install.packages('jsonlite', dependencies = T)
  library(jsonlite)
  
  if (!require(fastDummies)) install.packages('fastDummies', dependencies = T)
  library(fastDummies)
}

if (!require(caret)) install.packages('caret', dependencies = T)
library(caret)   

if (!require(ggplot2)) install.packages('ggplot2', dependencies = T)
library(ggplot2)

```


Examinamos la estructura y el aspecto del fichero importado:

```{r}
#See the structure
str(bank_raw)
```


```{r}
#Summary
summary(bank_raw)
```

```{r}
#see some records
head(bank_raw,5)
```


```{r}
#Summary
hist(bank_raw$age)
hist(bank_raw$balance)
hist(bank_raw$duration)
hist(bank_raw$campaign)
hist(bank_raw$pdays)
hist(bank_raw$previous)
```

La única variable que se aproxima a la distribución normal es la edad.
Ninguna se aproxima a la uniforme.

Así que normalizamos las variables numéricas de 0 a 1 con la ayuda de la función scale.
(No se normalizan ni los días ni los meses).

```{r}
#scale numeric variables 
maxs <- apply(bank_raw[c(1,6,12,13,14,15)], 2, max)
mins <- apply(bank_raw[c(1,6,12,13,14,15)], 2, min)

bank_norm <- data.frame(scale(bank_raw[c(1,6,12,13,14,15)], center = mins, scale = maxs - mins))
```

```{r}
# normalize numeric features
#bank_norm <- sapply(bank_raw, function(x) if(is.numeric(x)) {
#                                scale(x)
#                              } else x)
```


```{r}
#Summary
summary(bank_norm)
```


Ahora debemos transformar las variables categóricas en numéricas ("hot encoding").
La variable "month" he pensado transformarla en una sola variable:  Enero -> 1, Febrero -> 2 ...
Utilizar "hot enconding" con esta variable me parece que es añadir demasiadas variables sin necesidad.
(He leído que a las redes neuronales no les van demasiado bien las matrices dispersas).

```{r}
#hot encoding of categorical features         
dummies <- dummyVars(" ~ job + marital + education + default + housing + loan + contact + poutcome + y", data = bank_raw)  # y for neuralnet and RSNNS
bank_hot_encoded_feat <-  data.frame(predict(dummies, newdata = bank_raw))
head(bank_hot_encoded_feat,5)
```


Transformar los meses en una variable numérica.

```{r}
#encoding month (name to number)

#unique(bank_raw$month)  -> Levels: apr aug dec feb jan jul jun mar may nov oct sep

month_to_number <- function(month_name) {
  month_and_number <- c("jan"=1,"feb"=2,"mar"=3,"apr"=4,"may"=5,"jun"=6,"jul"=7,"aug"=8,"sep"=9,"oct"=10,"nov"=11,"dec"=12)
  return(month_and_number[as.character(month_name)])
}

#tests
month_to_number("oct")
month_to_number("may")

test <- bank_raw$month[1:5]
test
result <-sapply(test, month_to_number)
result

```


```{r}
bank_raw$month_num <- sapply(bank_raw$month, month_to_number)

#bank_raw$month_num <- as.integer(factor(bank_raw$month, levels = unique(bank_raw$month))) # codifica poniendo los números según aparecen en los levels, no Enero=1, Febrero=2 ...
head(bank_raw,5)
```

```{r}
#transform target categorical feature (keras)
dummy_y <- fastDummies::dummy_cols(bank_raw$y,remove_first_dummy = TRUE)

head(dummy_y)   
```


Juntamos todas las variables en un mismo dataframe.

```{r}
bank_processed <- cbind(bank_norm,as.numeric(bank_raw$day),bank_raw$month_num,bank_hot_encoded_feat,dummy_y$.data_yes)
names(bank_processed)[7:8] <- c("day","month")
names(bank_processed)[43] <- c("y")
head(bank_processed,5)
```


Finalmente, creamos los conjuntos de entrenamiento y validación:

```{r}
#Set seed to make the process reproducible
set.seed(9)

#partitioning data frame into training (75%) and testing (25%) sets
train_indices <- createDataPartition(bank_processed$y, times=1, p=.75, list=FALSE)

#create training set
bank_processed_train <- bank_processed[train_indices, ]

#create testing set
bank_processed_test  <- bank_processed[-train_indices, ]

  
if (library == "keras") {
  X_train_bank <- bank_processed_train %>% #select(-y,-y.yes,-y.no) %>%
    select(-y,-y.yes,-y.no) %>%  
    keras3::as_tensor(dtype = "float32")  
  
  y_train_bank <- keras3::to_categorical(bank_processed_train$y)
    
  X_test_bank <- bank_processed_test %>% 
    select(-y,-y.yes,-y.no) %>%  
    keras3::as_tensor(dtype = "float32")  

  y_test_bank <- keras3::to_categorical(bank_processed_test$y)
  
} else {
  X_train_bank <- bank_processed_train[ , -c(41,42,43)] 
  y_train_bank <- bank_processed_train[ , c(41,42)] 

  X_test_bank <- bank_processed_test[ , -c(41,42,43)]
  y_test_bank <- bank_processed_test[ , c(41,42)] 
}


#view number of rows in each set
nrow(X_train_bank)  # 3391
nrow(X_test_bank)   # 1130
nrow(y_train_bank)  # 3391
nrow(y_test_bank)   # 1130
```


## Paso 3: Entrenamiento del modelo


```{r}
# neuralnet
softplus <- function(x) { log(1 + exp(x)) }
```


```{r, echo = FALSE, message = FALSE}
set.seed(9)
knitr::opts_chunk$set(echo = FALSE)

if (library=="neuralnet") {
  print("Choosing neuralnet")
  
  system.time({
    model <- neuralnet(y.yes+y.no ~ .,  data = bank_processed_train[ , -which(names(bank_processed_train) %in% c("y"))], 
                       hidden = c(40,10,4), threshold = 0.05, lifesign="full") 
                       #act.fct = softplus, threshold = 0.01, algorithm = "backprop", learningrate=0.05 
  })
  
} else if  (library=="RSNNS") { 
  print("Choosing RSNNS")
  
  system.time({
      #model <- mlp(bank_processed_train[1:40], bank_processed_train[41:42], size = c(40,10,4), learnFuncParams = c(0.05), maxit = 1000)
      model <- mlp(X_train_bank, y_train_bank, size = c(40,10,4), learnFuncParams = c(0.05), maxit = 2000)
      # with hiddenActFunc=softplus, it never ends
  })
  
} else { #Keras
  print("Choosing Keras")
  
  model <- keras_model_sequential(name = "keras_mid_complex", input_shape = ncol(X_train_bank))
  model %>%
    layer_dense(name = "dense_1",units = 40, activation = 'relu') %>%
    layer_dropout(name = "droput_1", rate = 0.5) %>% 
    layer_dense(name = "dense_2",units = 20, activation = 'relu') %>%
    layer_dropout(name = "droput_2", rate = 0.3) %>% 
    layer_dense(name = "dense_3",units = 10, activation = 'relu') %>%
    layer_dropout(name = "droput_3", rate = 0.2) %>% 
    layer_dense(name = "dense_4",units = 4, activation = 'relu') %>%
    layer_dropout(name = "droput_4", rate = 0.1) %>% 
    layer_dense(name = "output_layer", units = 2, activation = 'sigmoid')
  
  model %>% compile(
    optimizer = "adam",               
    loss = "binary_crossentropy",
    metrics = 'accuracy'
  )
  
  #Training
  system.time({
    history <- model %>% fit(
      X_train_bank, y_train_bank, 
      epochs = 200, 
      batch_size = 40,
      validation_split = 0.2
    )
  })
  
}

```

Visualizamos la arquitectura de la red entrenada y sus pesos:

```{r, fig.width=100, fig.height=140}
# neuralnet
if (library=="neuralnet") {
  #plot(model)   #saved in file "Chapter07/neuralnet_10_neurons_model.png"
}
if (library=="keras") {
  model
  #plot(model)
}
```

```{r, fig.width=12, fig.height=6}
if (library=="keras") {
  plot(history)
}
```


En el caso de utilizar las librerías Keras&TensorFlow, podemos obtener un Json con información del modelo:

```{r}
if (library=="keras") {
  #WE can print a json with the info of the model:
  #prettify(keras::model_to_json(model))
}
```


[

Primeros resultados obtenidos con la librería neuralnet:

(0)
model <- neuralnet(y.yes+y.no ~ .,  data = bank_processed_train[ , -which(names(bank_processed_train) %in% c("y"))], hidden=1) 
hidden=1  
       User      System verstrichen 
       0.86        0.00        0.91 
       
(1)       
model <- neuralnet(y.yes+y.no ~ .,  data = bank_processed_train[ , -which(names(bank_processed_train) %in% c("y"))], hidden=10) 
Warning: Algorithm did not converge in 1 of 1 repetition(s) within the stepmax.       <- !!!!!!
       User      System verstrichen 
    484.682       1.604     489.933 
  
(2)    
model <- neuralnet(y.yes+y.no ~ .,  data = bank_processed_train[ , -which(names(bank_processed_train) %in% c("y"))], hidden = c(20,10), threshold = 0.05, algorithm = "rprop+") 
Warning: Algorithm did not converge in 1 of 1 repetition(s) within the stepmax.       
       User      System verstrichen 
   1158.842       2.555    1161.369
   
(3)
model <- neuralnet(y.yes+y.no ~ .,  data = bank_processed_train[ , -which(names(bank_processed_train) %in% c("y"))], hidden = c(10,2))   
Warning: Algorithm did not converge in 1 of 1 repetition(s) within the stepmax.       
       User      System verstrichen 
    487.436       0.162     487.498    
   
   
   
(4)
model <- neuralnet(y.yes+y.no ~ .,  data = bank_processed_train[ , -which(names(bank_processed_train) %in% c("y"))], hidden = c(10,2), threshold = 0.1, lifesign="full")
       User      System verstrichen 
    274.745       0.245     274.977    
    
(5)
model <- neuralnet(y.yes+y.no ~ .,  data = bank_processed_train[ , -which(names(bank_processed_train) %in% c("y"))], hidden = 20, algorithm = "rprop+", threshold = 0.5, lifesign="full")    
       User      System verstrichen 
    177.455       1.304     179.895 
    
(6)
model <- neuralnet(y.yes+y.no ~ .,  data = bank_processed_train[ , -which(names(bank_processed_train) %in% c("y"))], hidden = 43, threshold = 0.5, lifesign="full")     
    
(7)
Warning: Algorithm did not converge in 1 of 1 repetition(s) within the stepmax.       User      System verstrichen 
   2334.860       5.836    2356.510     
model <- neuralnet(y.yes+y.no ~ .,  data = bank_processed_train[ , -which(names(bank_processed_train) %in% c("y"))], hidden = 43,  
                   act.fct = softplus, threshold = 0.5, lifesign="full")   

]



## Paso 4: Evaluación del modelo


```{r}
if (library=="keras") {
  model %>% evaluate(X_test_bank, y_test_bank)
}
```


Una vez entrenado el modelo, pasamos a analizar su capacidad predictiva:

```{r}
# neuralnet
if (library=="neuralnet") {
  #prediction <- compute(model, bank_processed_test[ , -which(names(bank_processed_test) %in% c("y"))])  #compute is deprecated, we use predict
  predictions <- predict(model, bank_processed_test[ , -which(names(bank_processed_test) %in% c("y","y.yes","y.no"))])
  
} else if (library=="RSNNS") {
  
  predictions <- predict(model, X_test_bank)

} else {  #Keras
  
  predictions <- model %>% predict(X_test_bank)
  
}
```



Matriz de confusión:


```{r}
# neuralnet, RSNNS, keras

prediction <- apply(predictions,1,which.max)  #find which column has the highest value

prediction[prediction==1] <- "no"     #and translate that value to one of the two possible values
prediction[prediction==2] <- "yes"

y_test_bank_real <- apply(y_test_bank,1,which.max)  # the same for test data

y_test_bank_real[y_test_bank_real==1] <- "no"    
y_test_bank_real[y_test_bank_real==2] <- "yes"  
```


```{r}
# Confussion matrix

caret::confusionMatrix(as.factor(y_test_bank_real), as.factor(prediction), positive="yes", mode = "everything")

```



### Resultados con la librería neuralnet:

(0)
model <- neuralnet(y.yes+y.no ~ .,  data = bank_processed_train[ , -which(names(bank_processed_train) %in% c("y"))], hidden=1)

Confusion Matrix and Statistics

          Reference
Prediction  no yes
       no   35 958
       yes  48  89
                                          
               Accuracy : 0.1097          
                                          
       'Positive' Class : no    
       
       
Confusion Matrix and Statistics

          Reference
Prediction  no yes
       no   64 929
       yes  44  93       
       
(4)
model <- neuralnet(y.yes+y.no ~ .,  data = bank_processed_train[ , -which(names(bank_processed_train) %in% c("y"))], hidden = c(10,2), threshold = 0.1, lifesign="full")

Confusion Matrix and Statistics

          Reference
Prediction  no yes
       no   60 933
       yes  52  85
                                          
               Accuracy : 0.1283          
                                          
       'Positive' Class : no    
       
       

(5)
model <- neuralnet(y.yes+y.no ~ .,  data = bank_processed_train[ , -which(names(bank_processed_train) %in% c("y"))], hidden = 20, algorithm = "rprop+", threshold = 0.5, lifesign="full")    
       User      System verstrichen 
    177.455       1.304     179.895 
    
          Reference
Prediction  no yes
       no   61 932
       yes  38  99
                                          
               Accuracy : 0.1416     
    
(6)
model <- neuralnet(y.yes+y.no ~ .,  data = bank_processed_train[ , -which(names(bank_processed_train) %in% c("y"))], 
                        hidden = 40, threshold = 0.5, lifesign="full")    
       User      System verstrichen 
    245.882       2.815     250.844 

          Reference
Prediction  no yes
       no   88 905
       yes  51  86
                                          
               Accuracy : 0.154 
               

(7)
model <- neuralnet(y.yes+y.no ~ .,  data = bank_processed_train[ , -which(names(bank_processed_train) %in% c("y"))], 
                       hidden = c(40,10,4), threshold = 0.05, lifesign="full") 

  user  system elapsed 
768.950   0.619 769.704   <- almost 13 min !!!

          Reference
Prediction  no yes
       no   39 954
       yes  32 105
                                          
              Accuracy : 0.1274       <- !!!!!!    




### Resultados con la librería RSNNS:

(1)
model <- mlp(bank_processed_train[1:40],  bank_processed_train[41:42], size = c(10), learnFuncParams = c(0.1), maxit = 1000)
       predictions
targets   1   2
      1 964  29
      2 108  29

(2)
model <- mlp(bank_processed_train[1:40],  bank_processed_train[41:42], size = c(10), learnFuncParams = c(0.1), maxit = 10000)
       User      System verstrichen 
    122.006       0.306     123.809
   
          Reference
Prediction  no yes
       no  948  45
       yes 115  22
                                          
               Accuracy : 0.8584
                 95% CI : (0.8367, 0.8782)
    No Information Rate : 0.9407          
    P-Value [Acc > NIR] : 1               
                                          
                  Kappa : 0.1478          
                                          
 Mcnemar's Test P-Value : 4.899e-08       
                                          
            Sensitivity : 0.32836         
            Specificity : 0.89182         
         Pos Pred Value : 0.16058         
         Neg Pred Value : 0.95468         
             Prevalence : 0.05929         
         Detection Rate : 0.01947         
   Detection Prevalence : 0.12124         
      Balanced Accuracy : 0.61009         
                                          
       'Positive' Class : yes  
 
(3)
model <- mlp(bank_processed_train[1:40],  bank_processed_train[41:42], size = c(43,10), learnFuncParams = c(0.1), maxit = 10000)
       User      System verstrichen 
    681.628       0.759     687.135        

Confusion Matrix and Statistics

          Reference
Prediction  no yes
       no  931  62
       yes  81  56
                                          
               Accuracy : 0.8735          
                 95% CI : (0.8526, 0.8923)
    No Information Rate : 0.8956          
    P-Value [Acc > NIR] : 0.9923          
                                          
                  Kappa : 0.3683          
                                          
 Mcnemar's Test P-Value : 0.1323          
                                          
            Sensitivity : 0.47458         
            Specificity : 0.91996         
         Pos Pred Value : 0.40876         
         Neg Pred Value : 0.93756         
             Prevalence : 0.10442         
         Detection Rate : 0.04956         
   Detection Prevalence : 0.12124         
      Balanced Accuracy : 0.69727         
                                          
       'Positive' Class : yes 

  
(4) Ajustamos el número de neuronas de la primera capa oculta, y bajamos las iteraciones a 10000.
La predicción se hace en mucho menos tiempo (menos de la decima parte), y el resultado es algo mejor:

model <- mlp(bank_processed_train[1:40],  bank_processed_train[41:42], size = c(40,10), learnFuncParams = c(0.1),  maxit = 1000)  
       User      System verstrichen 
     46.186       0.050      46.445   
     
Confusion Matrix and Statistics

          Reference
Prediction  no yes
       no  975  18
       yes 117  20
                                          
               Accuracy : 0.8805          
                 95% CI : (0.8602, 0.8989)
    No Information Rate : 0.9664          
    P-Value [Acc > NIR] : 1               
                                          
                  Kappa : 0.1857          
                                          
 Mcnemar's Test P-Value : <2e-16          
                                          
            Sensitivity : 0.52632         
            Specificity : 0.89286         
         Pos Pred Value : 0.14599         
         Neg Pred Value : 0.98187         
             Prevalence : 0.03363         
         Detection Rate : 0.01770         
   Detection Prevalence : 0.12124         
      Balanced Accuracy : 0.70959         
                                          
       'Positive' Class : yes        
     
     
(5) No hay mejora al añadir 10 neuronas más en la segunda capa

model <- mlp(bank_processed_train[1:40],  bank_processed_train[41:42], size = c(40,20), learnFuncParams = c(0.1),  maxit = 1000)     
       User      System verstrichen 
     57.033       0.060      57.301  
     
          Reference
Prediction  no yes
       no  965  28
       yes 108  29
                                          
               Accuracy : 0.8796      
               
               
### (6) Sí mejora añadir una tercera capa con 4 neuronas!

model <- mlp(bank_processed_train[1:40], bank_processed_train[41:42], size = c(40,10,4), learnFuncParams = c(0.05), maxit = 1000)
       User      System verstrichen 
     45.978       0.058      46.650   Toshiba
       user      system elapsed 
     28.122      0.000  28.188        Lenovo
     
          Reference
Prediction  no yes
       no  953  40
       yes  89  48
                                          
               Accuracy : 0.8858 
               
     
     Probar finalmente:

(7)  Con 2 capas no mejora añadir iteraciones 
model <- mlp(bank_processed_train[1:40],  bank_processed_train[41:42], size = c(40,10), learnFuncParams = c(0.1),  maxit = 10000) 
       User      System verstrichen 
    643.455       0.429     645.196            
            
            Reference
Prediction  no yes
       no  955  38
       yes 102  35
                                          
               Accuracy : 0.8761 
               
               
(8) Sin añadir más capas, usar ReLu (softplus)
model <- mlp(bank_processed_train[1:40],  bank_processed_train[41:42], size = c(40,10), hiddenActFunc=softplus, learnFuncParams = c(0.1),  maxit = 1000) 

Cancelado. Llevaba más de 90 min. 


(9) Con una cuarta capa, mejora ligeramente el resultado de 3 capas (**mejor resultado de todos**)
model <- mlp(X_train_bank, y_train_bank, size = c(40,10,4,2), learnFuncParams = c(0.05), maxit = 2000)
   user  system elapsed 
 65.524   0.460  57.800 
 
Confusion Matrix and Statistics

          Reference
Prediction  no yes
       no  965  28
       yes  91  46
                                         
               Accuracy : 0.8947




### Resultados con la librería Keras&TensorFlow:


keras_complex:
-------------------------------------------------------------------------------------------

  model <- keras_model_sequential(name = "keras_complex", input_shape = ncol(X_train_bank))
  model %>%
    layer_dense(name = "layer_1",units = 40, activation = 'relu') %>%
    layer_dropout(name = "droput_2", rate = 0.4) %>% 
    layer_dense(name = "layer_3", units = 20, activation = 'relu') %>%
    layer_dropout(name = "droput_4", rate = 0.3) %>% 
    layer_dense(name = "layer_5", units = 10, activation = 'relu') %>%
    layer_dropout(name = "droput_6", rate = 0.15) %>% 
    layer_dense(name = "layer_7", units =  4, activation = 'relu') %>%
    layer_dense(name = "output_layer_8", units = 2, activation = 'sigmoid')
  
  model %>% compile(
    optimizer = "adam",               
    loss = "binary_crossentropy",
    metrics = 'accuracy'
  )
  
  #Training
  system.time({
    history <- model %>% fit(
      X_train_bank, y_train_bank, 
      epochs = 1000, 
      batch_size = 40,
      validation_split = 0.2
    )
  })
  
  
  Confusion Matrix and Statistics

          Reference
Prediction  no yes
       no  953  40
       yes  91  46
                                         
               Accuracy : 0.8841                  <- Quitando validación, la exactitud es 0.8717.  
                 95% CI : (0.864, 0.9022)            Probando con otros valores en las capas dropout, se obtiene 0.8788
    No Information Rate : 0.9239         
    P-Value [Acc > NIR] : 1              
                                         
                  Kappa : 0.352          
                                         
 Mcnemar's Test P-Value : 1.251e-05      
                                         
            Sensitivity : 0.53488        
            Specificity : 0.91284        
         Pos Pred Value : 0.33577        
         Neg Pred Value : 0.95972        
              Precision : 0.33577        
                 Recall : 0.53488        
                     F1 : 0.41256        
             Prevalence : 0.07611        
         Detection Rate : 0.04071        
   Detection Prevalence : 0.12124        
      Balanced Accuracy : 0.72386        
                                         
       'Positive' Class : yes 
       
  
keras_mid_complex:
-------------------------------------------------------------------------------------------     
       
  model <- keras_model_sequential(name = "keras_mid_complex", input_shape = ncol(X_train_bank))
  model %>%
    layer_dense(name = "layer_1",units = 40, activation = 'relu') %>%
    layer_dropout(name = "droput_2", rate = 0.4) %>% 

    layer_dense(name = "layer_5", units = 10, activation = 'relu') %>%
    layer_dropout(name = "droput_6", rate = 0.15) %>% 

    layer_dense(name = "output_layer_8", units = 2, activation = 'sigmoid')
  
  model %>% compile(
    optimizer = "adam",               
    loss = "binary_crossentropy",
    metrics = 'accuracy'
  )
  
  #Training
  system.time({
    history <- model %>% fit(
      X_train_bank, y_train_bank, 
      epochs = 1000, 
      batch_size = 40,
      validation_split = 0.2
    )
  })       
 
    user  system elapsed 
259.342  15.925 315.008 

Confusion Matrix and Statistics

          Reference
Prediction  no yes
       no  953  40
       yes  99  38
                                          
               Accuracy : 0.877           
                 95% CI : (0.8564, 0.8956)
    No Information Rate : 0.931           
    P-Value [Acc > NIR] : 1               
                                          
                  Kappa : 0.2911          
                                          
 Mcnemar's Test P-Value : 8.677e-07       
                                          
            Sensitivity : 0.48718         
            Specificity : 0.90589         
         Pos Pred Value : 0.27737         
         Neg Pred Value : 0.95972         
              Precision : 0.27737         
                 Recall : 0.48718         
                     F1 : 0.35349         
             Prevalence : 0.06903         
         Detection Rate : 0.03363         
   Detection Prevalence : 0.12124         
      Balanced Accuracy : 0.69654         
                                          
       'Positive' Class : yes  

 keras_mid_complex lots of neurons:
------------------------------------------------------------------------------------------- 

  model <- keras_model_sequential(name = "keras_mid_complex", input_shape = ncol(X_train_bank))
  model %>%
    layer_dense(name = "layer_1",units = 120, activation = 'relu') %>%
    layer_dropout(name = "droput_2", rate = 0.5) %>% 
    layer_dense(name = "layer_2",units = 40, activation = 'relu') %>%
    layer_dense(name = "output_layer", units = 2, activation = 'sigmoid')
  
  model %>% compile(
    optimizer = "adam",               
    loss = "binary_crossentropy",
    metrics = 'accuracy'
  )
  
  #Training
  system.time({
    history <- model %>% fit(
      X_train_bank, y_train_bank, 
      epochs = 1000, 
      batch_size = 40,
      validation_split = 0.2
    )
  })
  
          Reference
Prediction  no yes
       no  946  47
       yes  92  45
                                          
               Accuracy : 0.877

Mismo modelo, pero sin validación y con epoch=500:

          Reference
Prediction  no yes
       no  958  35
       yes  91  46
                                          
               Accuracy : 0.8885 


 keras_40_10_2:
-------------------------------------------------------------------------------------------       
      
  model <- keras_model_sequential(name = "keras_40_10_2", input_shape = ncol(X_train_bank))
  model %>%
    layer_dense(name = "layer_1",units = 40, activation = 'relu') %>%

    layer_dense(name = "layer_5", units = 10, activation = 'relu') %>%

    layer_dense(name = "output_layer_8", units = 2, activation = 'sigmoid')
  
  model %>% compile(
    optimizer = "adam",               
    loss = "binary_crossentropy",
    metrics = 'accuracy'
  )
  
  #Training
  system.time({
    history <- model %>% fit(
      X_train_bank, y_train_bank, 
      epochs = 1000, 
      batch_size = 40,
      validation_split = 0.2
    )
  })     
     
 user  system elapsed 
249.202  16.037 309.785 


Confusion Matrix and Statistics

          Reference
Prediction  no yes
       no  944  49
       yes  97  40
                                          
               Accuracy : 0.8708          
                 95% CI : (0.8498, 0.8898)
    No Information Rate : 0.9212          
    P-Value [Acc > NIR] : 1.0000000       
                                          
                  Kappa : 0.2858          
                                          
 Mcnemar's Test P-Value : 0.0001003       
                                          
            Sensitivity : 0.44944         
            Specificity : 0.90682         
         Pos Pred Value : 0.29197         
         Neg Pred Value : 0.95065         
              Precision : 0.29197         
                 Recall : 0.44944         
                     F1 : 0.35398         
             Prevalence : 0.07876         
         Detection Rate : 0.03540         
   Detection Prevalence : 0.12124         
      Balanced Accuracy : 0.67813         
                                          
       'Positive' Class : yes

keras_40_10_4_2
-------------------------------------------------------------------------------------------

  model <- keras_model_sequential(name = "keras_40_10_4_2", input_shape = ncol(X_train_bank))
  model %>%
    layer_dense(name = "layer_1",units = 40, activation = 'relu') %>%
    layer_dense(name = "layer_2", units = 10, activation = 'relu') %>%
    layer_dense(name = "layer_3", units = 4, activation = 'relu') %>%
    layer_dense(name = "output_layer_4", units = 2, activation = 'sigmoid')
  
  model %>% compile(
    optimizer = "adam",               
    loss = "binary_crossentropy",
    metrics = 'accuracy'
  )
  
  #Training
  system.time({
    history <- model %>% fit(
      X_train_bank, y_train_bank, 
      epochs = 10000, 
      batch_size = 40,
      validation_split = 0.2
    )
  })
    
user   system  elapsed 
2552.557  168.099 3210.650 

 Confusion Matrix and Statistics

          Reference
Prediction  no yes
       no  926  67
       yes  90  47
                                          
               Accuracy : 0.8611          
                 95% CI : (0.8395, 0.8807)
    No Information Rate : 0.8991          
    P-Value [Acc > NIR] : 0.99998         
                                          
                  Kappa : 0.2971          
                                          
 Mcnemar's Test P-Value : 0.07912         
                                          
            Sensitivity : 0.41228         
            Specificity : 0.91142         
         Pos Pred Value : 0.34307         
         Neg Pred Value : 0.93253         
              Precision : 0.34307         
                 Recall : 0.41228         
                     F1 : 0.37450         
             Prevalence : 0.10088         
         Detection Rate : 0.04159         
   Detection Prevalence : 0.12124         
      Balanced Accuracy : 0.66185         
                                          
       'Positive' Class : yes       



keras_20_2
-------------------------------------------------------------------------------------------

  model <- keras_model_sequential(name = "keras_20_2", input_shape = ncol(X_train_bank))
  model %>%
    layer_dense(name = "layer_1",units = 20, activation = 'relu') %>%

    layer_dense(name = "output_layer_8", units = 2, activation = 'sigmoid')
  
 
  user  system elapsed 
246.888  16.245 306.973   

Confusion Matrix and Statistics

          Reference
Prediction  no yes
       no  952  41
       yes  97  40
                                          
               Accuracy : 0.8779          
                 95% CI : (0.8574, 0.8964)
    No Information Rate : 0.9283          
    P-Value [Acc > NIR] : 1               
                                          
                  Kappa : 0.3043          
                                          
 Mcnemar's Test P-Value : 2.842e-06       
                                          
            Sensitivity : 0.49383         
            Specificity : 0.90753         
         Pos Pred Value : 0.29197         
         Neg Pred Value : 0.95871         
              Precision : 0.29197         
                 Recall : 0.49383         
                     F1 : 0.36697         
             Prevalence : 0.07168         
         Detection Rate : 0.03540         
   Detection Prevalence : 0.12124         
      Balanced Accuracy : 0.70068         
                                          
       'Positive' Class : yes

 
keras_20_2  exponential
-------------------------------------------------------------------------------------------   
    
  model <- keras_model_sequential(name = "keras_20_2", input_shape = ncol(X_train_bank))
  model %>%
    layer_dense(name = "layer_1",units = 20, activation = 'exponential') %>%

    layer_dense(name = "output_layer_8", units = 2, activation = 'sigmoid')
  
  model %>% compile(
    optimizer = "adam",               
    loss = "binary_crossentropy",
    metrics = 'accuracy'
  )
  
  #Training
  system.time({
    history <- model %>% fit(
      X_train_bank, y_train_bank, 
      epochs = 1000, 
      batch_size = 40,
      validation_split = 0.2
    )
  })


Confusion Matrix and Statistics

          Reference
Prediction  no yes
       no  931  62
       yes  94  43
                                          
               Accuracy : 0.8619  
               
               
Da igual el valor de epoch, el número de capas internas, y la función de activación.
Es como si hubiera un muro en el 88-89%.



 