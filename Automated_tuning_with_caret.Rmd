---
title: "Automated_tuning_with_caret"
author: "Fran Camacho"
date: "2025-04-24"
output:
  pdf_document: default
  word_document: default
  html_document: default
---



# Example – using caret for automated tuning

[Chapter 14 from the book "Machine Learning with R", by Brett Lantz].

Automated parameter tuning with caret will require you to consider three questions:

• What type of machine learning algorithm (and specific R implementation of this algorithm) 
should be trained on the data?

• Which hyperparameters can be adjusted for this algorithm, and how extensively should
they be tuned to find the optimal settings?

• What criterion should be used to evaluate the candidate models to identify the best overall
set of tuning values?


```{r}
if (!require(C50)) install.packages('C50', dependencies = T)  # Decision trees C5.0 algorithm
library(C50)

if (!require(caret)) install.packages('caret', dependencies = T)
library(caret)
```


```{r}
algor_list <- read.csv("./CSVs/algorithm_list.csv", sep = ";")
algor_list
```
[
For a complete list of the models and corresponding tuning options covered by caret,
refer to the table provided by package author Max Kuhn at 

  http://topepo.github.io/caret/available-models.html.
]

If you ever forget the tuning parameters for a particular model, the modelLookup() function can
be used to find them. Simply supply the method name as illustrated for the C5.0 model:

```{r}
modelLookup("C5.0")
```

By default, caret searches, at most, three values for each of the model’s p hyperparameters, 
which means that, at most, 3 p candidate models will be tested. For example, by default, 
the automatic tuning of k-nearest neighbors will compare 3 exp 1 = 3 candidate models 
with k=5, k=7, and k=9. Similarly, tuning a decision tree will result in a comparison of 
up to 27 different candidate models, comprising the grid of 3 exp 3 = 27 combinations of model, trials, and winnow settings.


### Creating a simple tuned model

Import data:

```{r}
credit <- read.csv("./CSVs/credit.csv", sep = ",")
```

Create simple model

To illustrate the process of tuning a model, let’s begin by observing what happens when we
attempt to tune the credit scoring model using the caret package’s default settings. The simplest
way to tune a learner requires only that you specify a model type via the method parameter.

Since we used C5.0 decision trees previously with the credit model, we’ll continue our work by
optimizing this learner. 

[

To create a model:

```{r}
# target must be a factor
credit$default <- factor(ifelse(credit$default=="yes","yes","no"))
```

```{r}
system.time({
simple_model <- C5.0(default ~ ., data = credit)
})
```
]

The basic train() command for tuning a C5.0 decision tree using the
default settings is as follows:

```{r}
set.seed(300)

system.time({
  m <- train(default ~ ., data = credit, method = "C5.0")
})
```

R must repeatedly generate random bootstrap
samples of data, build decision trees, compute performance statistics, and evaluate the result.
Because there are 12 candidate models with varying hyperparameter values to be evaluated, and
25 bootstrap samples per candidate model to compute an average performance measure, there
are 25*12 = 300 decision tree models being built using C5.0—and this doesn’t even count the
additional decision trees being built when the boosting trials are set!

To display the models calculated by caret train function:

```{r}
m
```
The final values used for the model were **trials = 20, model = tree and winnow = FALSE**.

```{r}
#str(m)
```

After identifying the best model, the train() function uses the tuned hyperparameters to build
a model on the full input dataset, which is stored in m as m$finalModel. In most cases, you will
not need to work directly with the finalModel sub-object. Instead, simply use the predict()
function with the m object as follows:

```{r}
p <- predict(m, credit)
```

The resulting vector of predictions works as expected, allowing us to create a confusion matrix
that compares the predicted and actual values:

```{r}
table(p, credit$default)
```

Of the 1,000 examples used for training the final model, only two were misclassified, for an accu-
racy of 99.8 percent. However, it is very important to note that since the model was built on both
the training and test data, this accuracy is optimistic and thus should not be viewed as indicative
of performance on unseen data.

In addition to automatic hyperparameter tuning, using the caret package’s train() and predict()
functions also offers a pair of benefits beyond the functions found in the stock packages.

i) First, any data preparation steps applied by the train() function will be similarly applied to the
data used for generating predictions. This includes transformations like centering and scaling,
as well as the imputation of missing values.

ii) Second, the predict() function provides a standardized interface for obtaining predicted class
values and predicted class probabilities, even for model types that ordinarily would require ad-
ditional steps to obtain this information. For a classification model, the predicted classes are
provided by default:

```{r}
head(predict(m, credit))
```

To obtain the estimated probabilities for each class, use the type = "prob" parameter:

```{r}
head(predict(m, credit, type = "prob"))
```

Even in cases where the underlying model refers to the prediction probabilities using a different
string (for example, "raw" for a naiveBayes model), the predict() function will translate 
**type = "prob"** to the appropriate parameter setting automatically.


### Customizing the tuning process

The **trainControl()** function is used to create a set of configuration options known as a control
object. This object guides the **train()** function and allows for the selection of model evaluation
criteria such as the resampling strategy and the measure used for choosing the best model. Although
this function can be used to modify nearly every aspect of a caret tuning experiment, we’ll focus 
on two important parameters: **method** and **selectionFunction**.

When using the trainControl() function, the method parameter sets the resampling method,
such as holdout sampling or k-fold CV. The following table lists the possible method values, as
well as any additional parameters for adjusting the sample size and the number of iteration:


```{r table2, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
train_control <- "
|+++++++++++++++++++++|++++++++++++++++|+++++++++++++++++++++++++++++++++++++++|
| Resampling method   | Method name     | Additional options and default values|
|+++++++++++++++++++++|+++++++++++++++++|++++++++++++++++++++++++++++++++++++++|
| Holdout sampling    | LGOCV           | p = 0.75 (training data proportion)  |
|---------------------|-----------------|--------------------------------------|
| k-fold CV           | cv              |   number = 10 (number of folds)      |
|---------------------|-----------------|--------------------------------------|
| Repeated k-fold CV  | repeatedcv      |   number = 10 (number of folds)      |
|                     |                 |   repeats = 10 (number of iterations)|
|---------------------|-----------------|--------------------------------------|
| Bootstrap sampling  |  boot           | number = 25 (resampling iterations)  |
|---------------------|-----------------|--------------------------------------|
| 0.632 bootstrap     |  boot63         | number = 25 (resampling iterations)  |
|---------------------|-----------------|--------------------------------------|
| Leave-one-out CV    |  LOOCV          | None                                 |
|---------------------|-----------------|--------------------------------------|
"
cat(train_control) # output the table in a format good for HTML/PDF/docx conversion
```

The **selectionFunction** parameter is used to specify the function that will choose the optimal
model among the candidates. Three such functions are included. 

The best function simply chooses the candidate with the best value on the specified performance measure. 
This is used by default. 

The other two functions are used to choose the most parsimonious, or simplest, model
that is within a certain threshold of the best model’s performance. 

The **oneSE** function chooses the simplest candidate within one standard error of the best performance, 
and **tolerance** uses the simplest candidate within a user-specified percentage.

To create a **control object** named ctrl that uses 10-fold CV and the oneSE selection function:

```{r}
ctrl <- trainControl(method = "cv", number = 10, selectionFunction = "oneSE")
```

We’ll use the result of this function shortly.

The next step in setting up our experiment is to create the search grid for hyperparameter tuning. 
The grid must include a column named for each hyperparameter in the desired model, 
regardless of whether it will be tuned. 
It must also include a row for each desired combination of values to test. 
We are using a C5.0 decision tree, so we’ll need columns named model, trials, and winnow.

Rather than filling the grid data frame cell by cell—a tedious task if there are many possible 
combinations of values—we can use the **expand.grid()** function:

```{r}
grid <- expand.grid(model = "tree",
                    trials = c(1, 5, 10, 15, 20, 25, 30, 35),
                    winnow = FALSE)
```

The resulting grid data frame contains 1*8*1 = 8 rows:

```{r}
grid
```

The train() function will build a candidate model for evaluation using each grid row’s 
combination of model parameters.


Given the search grid and the control object created previously, we are ready to run 
a thoroughly customized train() experiment.

```{r}
set.seed(300)

m <- train(default ~ ., data = credit, method = "C5.0",
            metric = "Kappa",
            trControl = ctrl,
            tuneGrid = grid)
```

This results in an object that we can view by typing its name:

```{r}
m
```

Although the output is similar to the automatically tuned model, there are a few notable differ-
ences. Because 10-fold CV was used, the sample size to build each candidate model was reduced
to 900 rather than the 1,000 used in the bootstrap. Furthermore, eight candidate models were
tested rather than the 12 in the prior experiment.

The best model here differs quite significantly from the prior experiment. Before, the best model
used trials = 20, whereas here, it used **trials = 5**. This change is because we used the oneSE
function rather than the best function to select the optimal model. Even though the model with
trials = 35 obtained the best kappa, the single-trial model offers reasonably close performance
with a much simpler algorithm.

[

Due to the large number of configuration parameters, caret can seem overwhelming at first. 
Don’t let this deter you: there is no easier way to test the performance of models using 10-fold CV. 

Instead, think of the experiment as defined by two parts:

- a **trainControl()** object that dictates the testing criteria, 
- and a tuning **grid** that determines what model parameters to evaluate. 

Supply these to the train() function and with a bit of computing time, the experiment will be complete.

]
