---
title: "Tema9_kmeans_usaArrest"
author: "Fran Camacho"
date: "2025-03-25"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

---
title: "Tema9_kmeans_uned_example"
author: "Fran Camacho"
date: "2025-03-15"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Tema 9 - K-Means Example

Taken from:

https://warin.ca/posts/rcourse-clustering-with-r/


## Step 1 – collecting data


```{r}
# import the dataset
arrest_raw <- as.data.frame(USArrests)
```


## Step 2 – exploring and preparing the data

Libraries

```{r}
if (!require(cluster)) install.packages('cluster', dependencies = T)
library(cluster)

if (!require(factoextra)) install.packages('factoextra', dependencies = T)
library(factoextra)

if (!require(magrittr)) install.packages('magrittr', dependencies = T)
library(magrittr)

if (!require(ggplot2)) install.packages('ggplot2', dependencies = T)
library(ggplot2)

if (!require(tidyr)) install.packages('tidyr', dependencies = T)
library(tidyr)

if (!require(dplyr)) install.packages('dplyr', dependencies = T)
library(dplyr)

if (!require(kableExtra)) install.packages('kableExtra', dependencies = T)
library(kableExtra)

```



```{r}
# str the dataset
str(arrest_raw)
```

```{r}
# summary
summary(arrest_raw)
```



```{r}

my_data <- arrest_raw %>%
  na.omit() %>%          # Remove missing values (NA)
  scale()                # Scale variables

```


```{r}
# summary
summary(my_data)
```

View data:

```{r}
kable(my_data) %>%
  kable_styling() %>%
  scroll_box(width = "600px", height = "300px")
```


### Distance measures

The common distance measures used for assessing similarity between observations.

**get_dist()**: for computing a distance matrix between the rows of a data matrix. 
Compared to the standard **dist()** function, it supports correlation-based distance measures including “pearson”, “kendall” and “spearman” methods.

**fviz_dist()**: for visualizing a distance matrix


```{r, fig.height=8}
res.dist <- get_dist(my_data, stand = TRUE, method = "pearson")

fviz_dist(res.dist, gradient = list(low = "#E0FFFF", mid = "white", high = "#FF4500"))
```

## Step 3 – training a model on the data


Determining the optimal number of clusters with **fviz_nbclust()**

fviz_nbclust() determines and visualizes the optimal number of clusters using different methods:
**within cluster sums of squares**, **average silhouette** and **gap statistics**.


- silhouette:

```{r, fig.height=5}

fviz_nbclust(my_data, kmeans, method = "silhouette")
```

2 clusters???

- wss:

```{r, fig.height=5}

fviz_nbclust(my_data, kmeans, method = "wss")
```

2 clusters? (1 makes no sense)


- gap_stat:

```{r, fig.height=5}

fviz_nbclust(my_data, kmeans, method = "gap_stat")
```

4 clusters? 2???


### Compute and visualize k-means clustering

**fviz_cluster()**: Visualize Clustering Results

K-means > One of the more popular algorithms for clustering is K-means. 
It divides the observations into discrete groups based on some distance metric.


```{r, fig.height=5}
set.seed(123)

km.res <- kmeans(my_data, 3, nstart = 25)

palette <- c("#255293", "#db0a16", "#f8c72d")

# Visualize
library(factoextra)

fviz_cluster(km.res, data = my_data,
             ellipse.type = "convex",
             palette = palette,
             ggtheme = theme_minimal())

```


## Hierarchical clustering

A tree-based representation of the objects, which is also known as dendrogram.


```{r, fig.height=5}
set.seed(123)

# Compute hierarchical clustering
res.hc <- USArrests %>%
  scale() %>%                    # Scale the data
  dist(method = "euclidean") %>% # Compute dissimilarity matrix
  hclust(method = "ward.D2")     # Compute hierachical clustering

```

Display the tree of clusters:

```{r, fig.height=8}
set.seed(123)

# Visualize using factoextra
# Cut in 4 groups and color by groups
fviz_dend(res.hc, k = 4, # Cut in four groups (for coloring?)
          cex = 0.5, # label size
          k_colors = c("#87CEEB", "#ADFF2F", "#FFD700", "#FF4500"),
          color_labels_by_k = TRUE, # color labels by groups
          rect = TRUE # Add rectangle around groups
          )

```


## K-medoids/pam clustering

Two problems with K-means clustering are that it does not work with categorical data and it is susceptible to outliers. 
An alternative is K-medoids. Instead of the center of a cluster being the mean of the cluster, 
the center is one of the actual observations in the cluster. 
This is akin to the median, which is likewise robust against outliers.
The most common K-medoids algorithm is Partitioning Around Medoids (PAM). 

```{r, fig.height=5}
set.seed(123)

# Compute PAM
pam.res <- pam(my_data, 3)

# Visualize
fviz_cluster(pam.res)

```